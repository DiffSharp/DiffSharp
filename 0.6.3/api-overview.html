<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <!-- 
      The API Overview
 parameters will be replaced with the 
      document title extracted from the <h1> element or
      file name, if there is no <h1> heading
    -->
    <title>API Overview
</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Atılım Güneş Baydin; Barak A. Pearlmutter">
    <meta name="description" content="DiffSharp is an automatic differentiation (AD) library implemented in the F# language by Atılım Güneş Baydin and Barak A. Pearlmutter, mainly for research applications in machine learning, as part of their work at the Brain and Computation Lab, Hamilton Institute, National University of Ireland Maynooth.">

    <script src="https://code.jquery.com/jquery-1.8.0.js"></script>
    <script src="https://code.jquery.com/ui/1.8.23/jquery-ui.js"></script>
    <script src="https://netdna.bootstrapcdn.com/twitter-bootstrap/2.2.1/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link href="https://netdna.bootstrapcdn.com/twitter-bootstrap/2.2.1/css/bootstrap-combined.min.css" rel="stylesheet">
    
    <link type="text/css" rel="stylesheet" href="misc/style.css" />
    <script src="misc/tips.js" type="text/javascript"></script>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-48900508-3', 'auto');
      ga('require', 'displayfeatures');
      ga('send', 'pageview');
    </script>
  </head>
  <body>
    <div class="container">
      <div class="masthead">
        <ul class="nav nav-pills pull-right">
          <li><a href="https://fsharp.org">fsharp.org</a></li>
        </ul>
        <h3 class="muted">DiffSharp</h3>
      </div>
      <hr />
      <div class="row">
        <div class="span9" id="main">
          <h1>API Overview</h1>

<p>The following table gives an overview of the differentiation API provided by the DiffSharp library.</p>

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg th{font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg .tg-a60z{font-size:10px;background-color:#ecf4ff}
.tg .tg-1r2d{font-size:10px;background-color:#ecf4ff;text-align:center}
.tg .tg-glis{font-size:10px}
.tg .tg-wcxf{font-size:10px;background-color:#ffffc7;text-align:center}
.tg .tg-aycn{font-size:10px;background-color:#e4ffb3;text-align:center}
.tg .tg-wklz{font-size:10px;background-color:#ecf4ff;color:#000000;text-align:center}
.tg .tg-7dqz{font-weight:bold;font-size:10px}
</style>
<table class="tg">
  <tr>
    <th class="tg-glis"></th>
    <th class="tg-wcxf">diff</th>
    <th class="tg-wcxf">diff2</th>
    <th class="tg-wcxf">diffn</th>
    <th class="tg-aycn">grad</th>
    <th class="tg-aycn">gradv</th>
    <th class="tg-aycn">hessian</th>
    <th class="tg-aycn">hessianv</th>
    <th class="tg-aycn">gradhessian</th>
    <th class="tg-aycn">gradhessianv</th>
    <th class="tg-aycn">laplacian</th>
    <th class="tg-wklz">jacobian</th>
    <th class="tg-1r2d">jacobianv</th>
    <th class="tg-1r2d">jacobianT</th>
    <th class="tg-1r2d">jacobianTv</th>
    <th class="tg-a60z">curl</th>
    <th class="tg-a60z">div</th>
    <th class="tg-a60z">curldiv</th>
  </tr>
  <tr>
    <td class="tg-7dqz">AD</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
  </tr>
  <tr>
    <td class="tg-7dqz">ADF</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
  </tr>
  <tr>
    <td class="tg-7dqz">ADR</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
  </tr>
  <tr>
    <td class="tg-7dqz">SADF1</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf"></td>
    <td class="tg-wcxf"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-wklz">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
  </tr>
  <tr>
    <td class="tg-7dqz">SADF2</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-wklz">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
  </tr>
  <tr>
    <td class="tg-7dqz">SADFG</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf"></td>
    <td class="tg-wcxf"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-wklz">X</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
  </tr>
  <tr>
    <td class="tg-7dqz">SADFGH</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf"></td>
    <td class="tg-wcxf"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-wklz">X</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
  </tr>
  <tr>
    <td class="tg-7dqz">SADFN</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
  </tr>
  <tr>
    <td class="tg-7dqz">SADR1</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf"></td>
    <td class="tg-wcxf"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-wklz">X</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
  </tr>
  <tr>
    <td class="tg-7dqz">N</td>
    <td class="tg-wcxf">A</td>
    <td class="tg-wcxf">A</td>
    <td class="tg-wcxf"></td>
    <td class="tg-aycn">A</td>
    <td class="tg-aycn">A</td>
    <td class="tg-aycn">A</td>
    <td class="tg-aycn">A</td>
    <td class="tg-aycn">A</td>
    <td class="tg-aycn">A</td>
    <td class="tg-aycn">A</td>
    <td class="tg-wklz">A</td>
    <td class="tg-1r2d">A</td>
    <td class="tg-1r2d">A</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">A</td>
    <td class="tg-1r2d">A</td>
    <td class="tg-1r2d">A</td>
  </tr>
  <tr>
    <td class="tg-7dqz">S</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-wklz">X</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
  </tr>
</table>

<p><strong>Yellow</strong>: For scalar-to-scalar functions; <strong>Green</strong>: For vector-to-scalar functions; <strong>Blue</strong>: For vector-to-vector functions</p>

<p><strong>X</strong>: Exact value; <strong>A</strong>: Numerical approximation</p>

<p>The main functionality in DiffSharp is provided by the <strong>DiffSharp.AD</strong> namespace, which supports nesting and can combine forward and reverse AD as needed. The modules under <strong>DiffSharp.AD.Specialized</strong> provide several non-nested implementations of AD, for situations where you know beforehand that nesting will not be needed. This can give better performance for some specific non-nested tasks.</p>

<p>The main focus of the DiffSharp library is AD, but we also implement symbolic and numerical differentiation.</p>

<p>Currently, the library provides the following implementations:</p>

<ul>
<li><strong>AD | DiffSharp.AD</strong>: Nested AD, forward and reverse mode</li>
<li><strong>ADF | DiffSharp.AD.Forward</strong>: Nested AD, only forward mode</li>
<li><strong>ADR | DiffSharp.AD.Reverse</strong>: Nested AD, only reverse mode</li>
<li><strong>SADF1 | DiffSharp.AD.Specialized.Forward1</strong>: Non-nested AD, forward mode, 1st order</li>
<li><strong>SADF2 | DiffSharp.AD.Specialized.Forward2</strong>: Non-nested AD, forward mode, 2nd order</li>
<li><strong>SADFG | DiffSharp.AD.Specialized.ForwardG</strong>: Non-nested AD, forward mode, keeping vectors of gradient components</li>
<li><strong>SADFGH | DiffSharp.AD.Specialized.ForwardGH</strong>: Non-nested AD, forward mode, keeping vectors of gradient components and matrices of Hessian components</li>
<li><strong>SADFN | DiffSharp.AD.Specialized.ForwardN</strong>: Non-nested AD, forward mode, lazy higher-order</li>
<li><strong>SADR1 | DiffSharp.AD.Specialized.Reverse1</strong>: Non-nested AD, reverse mode, 1st order</li>
<li><strong>N | DiffSharp.Numerical</strong>: Numerical differentiation</li>
<li><strong>S | DiffSharp.Symbolic</strong>: Symbolic differentiation</li>
</ul>

<p>For brief explanations of these implementations, please refer to the <a href="gettingstarted-nestedad.html">Nested AD</a>, <a href="gettingstarted-nonnestedad.html">Non-nested AD</a>, <a href="gettingstarted-numericaldifferentiation.html">Numerical Differentiation</a>, and <a href="gettingstarted-symbolicdifferentiation.html">Symbolic Differentiation</a> pages.</p>

<h2>Differentiation Operations and Variants</h2>

<p>The operations summarized in the above table have <em>prime-suffixed</em> variants (e.g. <strong>diff</strong> and <strong>diff'</strong> ) that return tuples containing the value of the original function together with the value of the desired operation. This provides a performance advantage in the majority of AD operations, since the original function value is almost always computed during the same execution of the code, as a by-product of AD computations.</p>

<table class="pre"><tr><td class="lines"><pre class="fssnip">
<span class="l">1: </span>
<span class="l">2: </span>
<span class="l">3: </span>
<span class="l">4: </span>
<span class="l">5: </span>
<span class="l">6: </span>
<span class="l">7: </span>
</pre>
</td>
<td class="snippet"><pre class="fssnip">
<span class="k">open</span> <span onmouseout="hideTip(event, 'fs1', 1)" onmouseover="showTip(event, 'fs1', 1)" class="i">DiffSharp</span><span class="o">.</span><span onmouseout="hideTip(event, 'fs2', 2)" onmouseover="showTip(event, 'fs2', 2)" class="i">AD</span>

<span class="c">// Derivative of Sin(Sqrt(x)) at x = 2</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs3', 3)" onmouseover="showTip(event, 'fs3', 3)" class="i">a</span> <span class="o">=</span> <span onmouseout="hideTip(event, 'fs4', 4)" onmouseover="showTip(event, 'fs4', 4)" class="f">diff</span> (<span class="k">fun</span> <span onmouseout="hideTip(event, 'fs5', 5)" onmouseover="showTip(event, 'fs5', 5)" class="i">x</span> <span class="k">-&gt;</span> <span onmouseout="hideTip(event, 'fs6', 6)" onmouseover="showTip(event, 'fs6', 6)" class="f">sin</span> (<span onmouseout="hideTip(event, 'fs7', 7)" onmouseover="showTip(event, 'fs7', 7)" class="f">sqrt</span> <span onmouseout="hideTip(event, 'fs5', 8)" onmouseover="showTip(event, 'fs5', 8)" class="i">x</span>)) (<span onmouseout="hideTip(event, 'fs8', 9)" onmouseover="showTip(event, 'fs8', 9)" class="p">D</span> <span class="n">2.</span>)

<span class="c">// (Original value, derivative) of Sin(Sqrt(x)) at x = 2</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs9', 10)" onmouseover="showTip(event, 'fs9', 10)" class="i">b</span>, <span onmouseout="hideTip(event, 'fs10', 11)" onmouseover="showTip(event, 'fs10', 11)" class="i">c</span> <span class="o">=</span> <span onmouseout="hideTip(event, 'fs11', 12)" onmouseover="showTip(event, 'fs11', 12)" class="f">diff&#39;</span> (<span class="k">fun</span> <span onmouseout="hideTip(event, 'fs5', 13)" onmouseover="showTip(event, 'fs5', 13)" class="i">x</span> <span class="k">-&gt;</span> <span onmouseout="hideTip(event, 'fs6', 14)" onmouseover="showTip(event, 'fs6', 14)" class="f">sin</span> (<span onmouseout="hideTip(event, 'fs7', 15)" onmouseover="showTip(event, 'fs7', 15)" class="f">sqrt</span> <span onmouseout="hideTip(event, 'fs5', 16)" onmouseover="showTip(event, 'fs5', 16)" class="i">x</span>)) (<span onmouseout="hideTip(event, 'fs8', 17)" onmouseover="showTip(event, 'fs8', 17)" class="p">D</span> <span class="n">2.</span>)
</pre>
</td>
</tr>
</table>

<table class="pre"><tr><td><pre><code>val a : D = D 0.05513442203
val c : D = D 0.05513442203
val b : D = D 0.987765946</code></pre></td></tr></table>

<p>Currently, the library provides the following operations:</p>

<h5>diff : <span class="math">\(\color{red}{(\mathbb{R} \to \mathbb{R}) \to \mathbb{R}} \to \color{blue}{\mathbb{R}}\)</span></h5>

<p><strong><code>diff f x</code></strong> returns the first derivative of a scalar-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<p>For a function <span class="math">\(f(a): \mathbb{R} \to \mathbb{R}\)</span>, and <span class="math">\(x \in \mathbb{R}\)</span>, this gives the derivative evaluated at <span class="math">\(x\)</span></p>

<p><span class="math">\[  \left. \frac{d}{da} f(a) \right|_{a\; =\; x} \; .\]</span></p>

<hr />

<h5>diff' : <span class="math">\(\color{red}{(\mathbb{R} \to \mathbb{R}) \to \mathbb{R}} \to \color{blue}{(\mathbb{R} \times \mathbb{R})}\)</span></h5>

<p><strong><code>diff' f x</code></strong> returns the original value and the first derivative of a scalar-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<hr />

<h5>diff2 : <span class="math">\(\color{red}{(\mathbb{R} \to \mathbb{R}) \to \mathbb{R}} \to \color{blue}{\mathbb{R}}\)</span></h5>

<p><strong><code>diff2 f x</code></strong> returns the second derivative of a scalar-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<p>For a function <span class="math">\(f(a): \mathbb{R} \to \mathbb{R}\)</span>, and <span class="math">\(x \in \mathbb{R}\)</span>, this gives the second derivative evaluated at <span class="math">\(x\)</span></p>

<p><span class="math">\[  \left. \frac{d^2}{da^2} f(a) \right|_{a\; =\; x} \; .\]</span></p>

<hr />

<h5>diff2' : <span class="math">\(\color{red}{(\mathbb{R} \to \mathbb{R}) \to \mathbb{R}} \to \color{blue}{(\mathbb{R} \times \mathbb{R})}\)</span></h5>

<p><strong><code>diff2' f x</code></strong> returns the original value and the second derivative of a scalar-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<hr />

<h5>diff2'' : <span class="math">\(\color{red}{(\mathbb{R} \to \mathbb{R}) \to \mathbb{R}} \to \color{blue}{(\mathbb{R} \times \mathbb{R} \times \mathbb{R})}\)</span></h5>

<p><strong><code>diff2'' f x</code></strong> returns the original value, the first derivative, and the second derivative of a scalar-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<hr />

<h5>diffn : <span class="math">\(\color{red}{\mathbb{R} \to (\mathbb{R} \to \mathbb{R}) \to \mathbb{R}} \to \color{blue}{\mathbb{R}}\)</span></h5>

<p><strong><code>diffn n f x</code></strong> returns the <code>n</code>-th derivative of a scalar-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<p>For <span class="math">\(n \in \mathbb{N}\)</span>, a function <span class="math">\(f(a): \mathbb{R} \to \mathbb{R}\)</span>, and <span class="math">\(x \in \mathbb{R}\)</span>, this gives the n-th derivative evaluated at <span class="math">\(x\)</span></p>

<p><span class="math">\[  \left. \frac{d^n}{da^n} f(a) \right|_{a\; =\; x} \; .\]</span></p>

<hr />

<h5>diffn' : <span class="math">\(\color{red}{\mathbb{R} \to (\mathbb{R} \to \mathbb{R}) \to \mathbb{R}} \to \color{blue}{(\mathbb{R} \times \mathbb{R})}\)</span></h5>

<p><strong><code>diffn' n f x</code></strong> returns the original value and the <code>n</code>-th derivative of a scalar-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<hr />

<h5>grad : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n} \to \color{blue}{\mathbb{R}^n}\)</span></h5>

<p><strong><code>grad f x</code></strong> returns the <a href="https://en.wikipedia.org/wiki/Gradient">gradient</a> of a vector-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<p>For a function <span class="math">\(f(a_1, \dots, a_n): \mathbb{R}^n \to \mathbb{R}\)</span>, and <span class="math">\(\mathbf{x} \in \mathbb{R}^n\)</span>, this gives the gradient evaluated at <span class="math">\(\mathbf{x}\)</span></p>

<p><span class="math">\[  \left( \nabla f \right)_\mathbf{x} = \left. \left[ \frac{\partial f}{{\partial a}_1}, \dots, \frac{\partial f}{{\partial a}_n} \right] \right|_{\mathbf{a}\; = \; \mathbf{x}} \; .\]</span></p>

<hr />

<h5>grad' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R} \times \mathbb{R}^n)}\)</span></h5>

<p><strong><code>grad' f x</code></strong> returns the original value and the gradient of a vector-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<hr />

<h5>gradv : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n \to \mathbb{R}^n} \to \color{blue}{\mathbb{R}}\)</span></h5>

<p><strong><code>gradv f x v</code></strong> returns the <a href="https://en.wikipedia.org/wiki/Directional_derivative">gradient-vector product</a> (directional derivative) of a vector-to-scalar function <code>f</code>, at the point <code>x</code>, along the vector <code>v</code>.</p>

<p>For a function <span class="math">\(f: \mathbb{R}^n \to \mathbb{R}\)</span>, and <span class="math">\(\mathbf{x}, \mathbf{v} \in \mathbb{R}^n\)</span>, this gives the dot product of the gradient of <span class="math">\(f\)</span> at <span class="math">\(\mathbf{x}\)</span> with <span class="math">\(\mathbf{v}\)</span></p>

<p><span class="math">\[  \left( \nabla f \right)_\mathbf{x} \cdot \mathbf{v} \; .\]</span></p>

<p>This value can be computed efficiently by the <strong>DiffSharp.AD.Forward</strong> module in one forward evaluation of the function, without computing the full gradient.</p>

<hr />

<h5>gradv' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R} \times \mathbb{R})}\)</span></h5>

<p><strong><code>gradv' f x v</code></strong> returns the original value and the gradient-vector product (directional derivative) of a vector-to-scalar function <code>f</code>, at the point <code>x</code>, along the vector <code>v</code>.</p>

<hr />

<h5>hessian : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n} \to \color{blue}{\mathbb{R}^{n \times n}}\)</span></h5>

<p><strong><code>hessian f x</code></strong> returns the <a href="https://en.wikipedia.org/wiki/Hessian_matrix">Hessian</a> of a vector-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<p>For a function <span class="math">\(f(a_1, \dots, a_n): \mathbb{R}^n \to \mathbb{R}\)</span>, and <span class="math">\(\mathbf{x} \in \mathbb{R}^n\)</span>, this gives the Hessian matrix evaluated at <span class="math">\(\mathbf{x}\)</span></p>

<p><span class="math">\[  \left( \mathbf{H}_f \right)_\mathbf{x} = \left. \begin{bmatrix}
                                           \frac{\partial ^2 f}{\partial a_1^2} &amp; \frac{\partial ^2 f}{\partial a_1 \partial a_2} &amp; \cdots &amp; \frac{\partial ^2 f}{\partial a_1 \partial a_n} \\
                                           \frac{\partial ^2 f}{\partial a_2 \partial a_1} &amp; \frac{\partial ^2 f}{\partial a_2^2} &amp; \cdots &amp; \frac{\partial ^2 f}{\partial a_2 \partial a_n} \\
                                           \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
                                           \frac{\partial ^2 f}{\partial a_n \partial a_1} &amp; \frac{\partial ^2 f}{\partial a_n \partial a_2} &amp; \cdots &amp; \frac{\partial ^2 f}{\partial a_n^2}
                                          \end{bmatrix} \right|_{\mathbf{a}\; = \; \mathbf{x}} \; .\]</span></p>

<hr />

<h5>hessian' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R} \times \mathbb{R}^{n \times n})}\)</span></h5>

<p><strong><code>hessian' f x</code></strong> returns the original value and the Hessian of a vector-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<hr />

<h5>hessianv : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n \to \mathbb{R}^n} \to \color{blue}{\mathbb{R}^n}\)</span></h5>

<p><strong><code>hessianv f x v</code></strong> returns the <a href="https://en.wikipedia.org/wiki/Hessian_automatic_differentiation">Hessian-vector product</a> of a vector-to-scalar function <code>f</code>, at the point <code>x</code>, along the vector <code>v</code>.</p>

<p>For a function <span class="math">\(f: \mathbb{R}^n \to \mathbb{R}\)</span>, and <span class="math">\(\mathbf{x}, \mathbf{v} \in \mathbb{R}^n\)</span>, this gives the multiplication of the Hessian matrix of <span class="math">\(f\)</span> at <span class="math">\(\mathbf{x}\)</span> with <span class="math">\(\mathbf{v}\)</span></p>

<p><span class="math">\[  \left( \mathbf{H}_f \right)_\mathbf{x} \; \mathbf{v} \; .\]</span></p>

<p>This value can be computed efficiently by the <strong>DiffSharp.AD.ForwardReverse</strong> module using one forward and one reverse evaluation of the function, in a matrix-free way (without computing the full Hessian matrix).</p>

<hr />

<h5>hessianv' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R} \times \mathbb{R}^n)}\)</span></h5>

<p><strong><code>hessianv' f x v</code></strong> returns the original value and the Hessian-vector product of a vector-to-scalar function <code>f</code>, at the point <code>x</code>, along the vector <code>v</code>.</p>

<hr />

<h5>gradhessian : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R}^n \times \mathbb{R}^{n \times n})}\)</span></h5>

<p><strong><code>gradhessian f x</code></strong> returns the gradient and the Hessian of a vector-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<hr />

<h5>gradhessian' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R} \times \mathbb{R}^n \times \mathbb{R}^{n \times n})}\)</span></h5>

<p><strong><code>gradhessian' f x</code></strong> returns the original value, the gradient, and the Hessian of a vector-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<hr />

<h5>gradhessianv : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R} \times \mathbb{R}^n)}\)</span></h5>

<p><strong><code>gradhessianv f x v</code></strong> returns the gradient-vector product (directional derivative) and the Hessian-vector product of a vector-to-scalar function <code>f</code>, at the point <code>x</code>, along the vector <code>v</code>.</p>

<hr />

<h5>gradhessianv' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R} \times \mathbb{R} \times \mathbb{R}^n)}\)</span></h5>

<p><strong><code>gradhessianv' f x v</code></strong> returns the original value, the gradient-vector product (directional derivative), and the Hessian-vector product of a vector-to-scalar function <code>f</code>, at the point <code>x</code>, along the vector <code>v</code>.</p>

<hr />

<h5>laplacian : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n} \to \color{blue}{\mathbb{R}}\)</span></h5>

<p><strong><code>laplacian f x</code></strong> returns the <a href="https://en.wikipedia.org/wiki/Laplace_operator#Laplace.E2.80.93Beltrami_operator">Laplacian</a> of a vector-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<p>For a function <span class="math">\(f(a_1, \dots, a_n): \mathbb{R}^n \to \mathbb{R}\)</span>, and <span class="math">\(\mathbf{x} \in \mathbb{R}^n\)</span>, this gives the sum of second derivatives evaluated at <span class="math">\(\mathbf{x}\)</span></p>

<p><span class="math">\[  \mathrm{tr}\left(\mathbf{H}_f \right)_\mathbf{x} = \left. \left(\frac{\partial ^2 f}{\partial a_1^2} + \dots + \frac{\partial ^2 f}{\partial a_n^2}\right) \right|_{\mathbf{a} \; = \; \mathbf{x}} \; ,\]</span></p>

<p>which is the trace of the Hessian matrix.</p>

<p>This value can be computed efficiently by the <strong>DiffSharp.AD.Forward2</strong> module, without computing the full Hessian matrix.</p>

<hr />

<h5>laplacian' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R} \times \mathbb{R})}\)</span></h5>

<p><strong><code>laplacian' f x</code></strong> returns the original value and the Laplacian of a vector-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<hr />

<h5>jacobian : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^m) \to \mathbb{R}^n} \to \color{blue}{\mathbb{R}^{m \times n}}\)</span></h5>

<p><strong><code>jacobian f x</code></strong> returns the <a href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant">Jacobian</a> of a vector-to-vector function <code>f</code>, at the point <code>x</code>.</p>

<p>For a function <span class="math">\(\mathbf{F}: \mathbb{R}^n \to \mathbb{R}^m\)</span> with components <span class="math">\(F_1 (a_1, \dots, a_n), \dots, F_m (a_1, \dots, a_n)\)</span>, and <span class="math">\(\mathbf{x} \in \mathbb{R}^n\)</span>, this gives the <span class="math">\(m\)</span>-by-<span class="math">\(n\)</span> Jacobian matrix evaluated at <span class="math">\(\mathbf{x}\)</span></p>

<p><span class="math">\[  \left( \mathbf{J}_\mathbf{F} \right)_\mathbf{x} = \left. \begin{bmatrix}
                                                            \frac{\partial F_1}{\partial a_1} &amp; \cdots &amp; \frac{\partial F_1}{\partial a_n} \\
                                                            \vdots &amp; \ddots &amp; \vdots  \\
                                                            \frac{\partial F_m}{\partial a_1} &amp; \cdots &amp; \frac{\partial F_m}{\partial a_n}
                                                           \end{bmatrix} \right|_{\mathbf{a}\; = \; \mathbf{x}} \; .\]</span></p>

<hr />

<h5>jacobian' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^m) \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R}^m \times \mathbb{R}^{m \times n})}\)</span></h5>

<p><strong><code>jacobian' f x</code></strong> returns the original value and the Jacobian of a vector-to-vector function <code>f</code>, at the point <code>x</code>.</p>

<hr />

<h5>jacobianv : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^m) \to \mathbb{R}^n \to \mathbb{R}^n} \to \color{blue}{\mathbb{R}^m}\)</span></h5>

<p><strong><code>jacobianv f x v</code></strong> returns the Jacobian-vector product of a vector-to-vector function <code>f</code>, at the point <code>x</code>, along the vector <code>v</code>.</p>

<p>For a function <span class="math">\(\mathbf{F}: \mathbb{R}^n \to \mathbb{R}^m\)</span>, and <span class="math">\(\mathbf{x}, \mathbf{v} \in \mathbb{R}^n\)</span>, this gives matrix product of the Jacobian of <span class="math">\(\mathbf{F}\)</span> at <span class="math">\(\mathbf{x}\)</span> with <span class="math">\(\mathbf{v}\)</span></p>

<p><span class="math">\[  \left( \mathbf{J}_\mathbf{F} \right)_\mathbf{x} \mathbf{v} \; .\]</span></p>

<p>This value can be computed efficiently by the <strong>DiffSharp.AD.Forward</strong> module in one forward evaluation of the function, in a matrix-free way (without computing the full Jacobian matrix).</p>

<hr />

<h5>jacobianv' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^m) \to \mathbb{R}^n \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R}^m \times \mathbb{R}^m)}\)</span></h5>

<p><strong><code>jacobianv' f x v</code></strong> returns the original value and the Jacobian-vector product of a vector-to-vector function <code>f</code>, at the point <code>x</code>, along the vector <code>v</code>.</p>

<hr />

<h5>jacobianT : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^m) \to \mathbb{R}^n} \to \color{blue}{\mathbb{R}^{n \times m}}\)</span></h5>

<p><strong><code>jacobianT f x</code></strong> returns the transposed Jacobian of a vector-to-vector function <code>f</code>, at the point <code>x</code>.</p>

<p>For a function <span class="math">\(\mathbf{F}: \mathbb{R}^n \to \mathbb{R}^m\)</span> with components <span class="math">\(F_1 (a_1, \dots, a_n), \dots, F_m (a_1, \dots, a_n)\)</span>, and <span class="math">\(\mathbf{x} \in \mathbb{R}^n\)</span>, this gives the <span class="math">\(n\)</span>-by-<span class="math">\(m\)</span> transposed Jacobian matrix evaluated at <span class="math">\(\mathbf{x}\)</span></p>

<p><span class="math">\[  \left( \mathbf{J}_\mathbf{F}^\textrm{T} \right)_\mathbf{x} = \left. \begin{bmatrix}
                                                            \frac{\partial F_1}{\partial a_1} &amp; \cdots &amp; \frac{\partial F_m}{\partial a_1} \\
                                                            \vdots &amp; \ddots &amp; \vdots  \\
                                                            \frac{\partial F_1}{\partial a_n} &amp; \cdots &amp; \frac{\partial F_m}{\partial a_n}
                                                           \end{bmatrix} \right|_{\mathbf{a}\; = \; \mathbf{x}} \; .\]</span></p>

<hr />

<h5>jacobianT' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^m) \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R}^m \times \mathbb{R}^{n \times m})}\)</span></h5>

<p><strong><code>jacobianT' f x</code></strong> returns the original value and the transposed Jacobian of a vector-to-vector function <code>f</code>, at the point <code>x</code>.</p>

<hr />

<h5>jacobianTv : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^m) \to \mathbb{R}^n \to \mathbb{R}^m} \to \color{blue}{\mathbb{R}^n}\)</span></h5>

<p><strong><code>jacobianTv f x v</code></strong> returns the transposed Jacobian-vector product of a vector-to-vector function <code>f</code>, at the point <code>x</code>, along the vector <code>v</code>.</p>

<p>For a function <span class="math">\(\mathbf{F}: \mathbb{R}^n \to \mathbb{R}^m\)</span>, <span class="math">\(\mathbf{x} \in \mathbb{R}^n\)</span>, and <span class="math">\(\mathbf{v} \in \mathbb{R}^m\)</span>, this gives the matrix product of the transposed Jacobian of <span class="math">\(\mathbf{F}\)</span> at <span class="math">\(\mathbf{x}\)</span> with <span class="math">\(\mathbf{v}\)</span></p>

<p><span class="math">\[  \left( \mathbf{J}_\mathbf{F}^\textrm{T} \right)_\mathbf{x} \mathbf{v} \; .\]</span></p>

<p>This value can be computed efficiently by the <strong>DiffSharp.AD.Reverse</strong> module in one forward and one reverse evaluation of the function, in a matrix-free way (without computing the full Jacobian matrix).</p>

<hr />

<h5>jacobianTv' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^m) \to \mathbb{R}^n \to \mathbb{R}^m} \to \color{blue}{(\mathbb{R}^m \times \mathbb{R}^n)}\)</span></h5>

<p><strong><code>jacobianTv' f x v</code></strong> returns the original value and the transposed Jacobian-vector product of a vector-to-vector function <code>f</code>, at the point <code>x</code>, along the vector <code>v</code>.</p>

<hr />

<h5>jacobianTv'' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^m) \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R}^m \times (\mathbb{R}^m \to \mathbb{R}^n))}\)</span></h5>

<p><strong><code>jacobianTv'' f x</code></strong> returns the original value and a function for evaluating the transposed Jacobian-vector product of a vector-to-vector function <code>f</code>, at point <code>x</code>.</p>

<p>Of the returned pair, the first is the original value of <code>f</code> at the point <code>x</code> (the result of the forward pass of the reverse mode AD) and the second is a function (the reverse evaluator) that can be used to compute the transposed Jacobian-vector product many times along many different vectors (performing a new reverse pass of the reverse mode AD, with the given vector, without repeating the forward pass).</p>

<p>This can be computed efficiently by the <strong>DiffSharp.AD.Reverse</strong> module in a matrix-free way (without computing the full Jacobian matrix).</p>

<h5>curl : <span class="math">\(\color{red}{(\mathbb{R}^3 \to \mathbb{R}^3) \to \mathbb{R}^3} \to \color{blue}{\mathbb{R}^3}\)</span></h5>

<p><strong><code>curl f x</code></strong> returns the <a href="https://en.wikipedia.org/wiki/Curl_(mathematics)">curl</a> of a vector-to-vector function <code>f</code>, at the point <code>x</code>.</p>

<p>For a function <span class="math">\(\mathbf{F}: \mathbb{R}^3 \to \mathbb{R}^3\)</span> with components <span class="math">\(F_1(a_1, a_2, a_3),\; F_2(a_1, a_2, a_3),\; F_3(a_1, a_2, a_3)\)</span>, and <span class="math">\(\mathbf{x} \in \mathbb{R}^3\)</span>, this gives</p>

<p><span class="math">\[  \left( \textrm{curl} \, \mathbf{F} \right)_{\mathbf{x}} = \left( \nabla \times \mathbf{F} \right)_{\mathbf{x}}= \left. \left[ \frac{\partial F_3}{\partial a_2} - \frac{\partial F_2}{\partial a_3}, \; \frac{\partial F_1}{\partial a_3} - \frac{\partial F_3}{\partial a_1}, \; \frac{\partial F_2}{\partial a_1} - \frac{\partial F_1}{\partial a_2} \right] \right|_{\mathbf{a}\; = \; \mathbf{x}} \; .\]</span></p>

<h5>curl' : <span class="math">\(\color{red}{(\mathbb{R}^3 \to \mathbb{R}^3) \to \mathbb{R}^3} \to \color{blue}{(\mathbb{R}^3 \times \mathbb{R}^3)}\)</span></h5>

<p><strong><code>curl' f x</code></strong> returns the original value and the curl of a vector-to-vector function <code>f</code>, at the point <code>x</code>.</p>

<h5>div : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^n) \to \mathbb{R}^n} \to \color{blue}{\mathbb{R}}\)</span></h5>

<p><strong><code>div f x</code></strong> returns the <a href="https://en.wikipedia.org/wiki/Divergence">divergence</a> of a vector-to-vector function <code>f</code>, at the point <code>x</code>.</p>

<p>For a function <span class="math">\(\mathbf{F}: \mathbb{R}^n \to \mathbb{R}^n\)</span> with components <span class="math">\(F_1(a_1, \dots, a_n),\; \dots, \; F_n(a_1, \dots, a_n)\)</span>, and <span class="math">\(\mathbf{x} \in \mathbb{R}^n\)</span>, this gives</p>

<p><span class="math">\[  \left( \textrm{div} \, \mathbf{F} \right)_{\mathbf{x}} = \left( \nabla \cdot \mathbf{F} \right)_{\mathbf{x}} = \textrm{tr}\left( \mathbf{J}_{\mathbf{F}} \right)_{\mathbf{x}} = \left. \left( \frac{\partial F_1}{\partial a_1} + \dots + \frac{\partial F_n}{\partial a_n}\right) \right|_{\mathbf{a}\; = \; \mathbf{x}} \; ,\]</span></p>

<p>which is the trace of the Jacobian matrix evaluated at <span class="math">\(\mathbf{x}\)</span>.</p>

<h5>div' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^n) \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R}^n \times \mathbb{R})}\)</span></h5>

<p><strong><code>div' f x</code></strong> returns the original value and the divergence of a vector-to-vector function <code>f</code>, at the point <code>x</code>.</p>

<h5>curldiv : <span class="math">\(\color{red}{(\mathbb{R}^3 \to \mathbb{R}^3) \to \mathbb{R}^3} \to \color{blue}{(\mathbb{R}^3 \times \mathbb{R})}\)</span></h5>

<p><strong><code>curldiv f x</code></strong> returns the curl and the divergence of a vector-to-vector function <code>f</code>, at the point <code>x</code>.</p>

<h5>curldiv' : <span class="math">\(\color{red}{(\mathbb{R}^3 \to \mathbb{R}^3) \to \mathbb{R}^3} \to \color{blue}{(\mathbb{R}^3 \times \mathbb{R}^3 \times \mathbb{R})}\)</span></h5>

<p><strong><code>curldiv' f x</code></strong> returns the original value, the curl, and the divergence of a vector-to-vector function <code>f</code>, at the point <code>x</code>.</p>

<h2>Vector and Matrix versus float[] and float[,]</h2>

<p>When a differentiation module such as <strong>DiffSharp.AD</strong> is opened, the default operations involving vectors or matrices handle these via <strong>float[]</strong> and <strong>float[,]</strong> arrays.</p>

<table class="pre"><tr><td class="lines"><pre class="fssnip">
<span class="l"> 1: </span>
<span class="l"> 2: </span>
<span class="l"> 3: </span>
<span class="l"> 4: </span>
<span class="l"> 5: </span>
<span class="l"> 6: </span>
<span class="l"> 7: </span>
<span class="l"> 8: </span>
<span class="l"> 9: </span>
<span class="l">10: </span>
<span class="l">11: </span>
<span class="l">12: </span>
<span class="l">13: </span>
<span class="l">14: </span>
<span class="l">15: </span>
<span class="l">16: </span>
<span class="l">17: </span>
<span class="l">18: </span>
<span class="l">19: </span>
<span class="l">20: </span>
</pre>
</td>
<td class="snippet"><pre class="fssnip">
<span class="k">open</span> <span onmouseout="hideTip(event, 'fs1', 19)" onmouseover="showTip(event, 'fs1', 19)" class="i">DiffSharp</span><span class="o">.</span><span onmouseout="hideTip(event, 'fs2', 20)" onmouseover="showTip(event, 'fs2', 20)" class="i">AD</span>

<span class="c">// Gradient of a vector-to-scalar function</span>
<span class="c">// g1: D[] -&gt; D[]</span>
<span class="c">// i.e., take the function arguments as a D[] and return the gradient as a D[]</span>
<span class="c">// Inner lambda expression: D[] -&gt; D</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs13', 21)" onmouseover="showTip(event, 'fs13', 21)" class="f">g1</span> <span class="o">=</span> <span onmouseout="hideTip(event, 'fs14', 22)" onmouseover="showTip(event, 'fs14', 22)" class="f">grad</span> (<span class="k">fun</span> <span onmouseout="hideTip(event, 'fs15', 23)" onmouseover="showTip(event, 'fs15', 23)" class="i">x</span> <span class="k">-&gt;</span> <span onmouseout="hideTip(event, 'fs6', 24)" onmouseover="showTip(event, 'fs6', 24)" class="f">sin</span> (<span onmouseout="hideTip(event, 'fs15', 25)" onmouseover="showTip(event, 'fs15', 25)" class="i">x</span><span class="o">.</span>[<span class="n">0</span>] <span class="o">*</span> <span onmouseout="hideTip(event, 'fs15', 26)" onmouseover="showTip(event, 'fs15', 26)" class="i">x</span><span class="o">.</span>[<span class="n">1</span>]))

<span class="c">// Compute the gradient at (1, 2)</span>
<span class="c">// g1val: D[]</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs16', 27)" onmouseover="showTip(event, 'fs16', 27)" class="i">g1val</span> <span class="o">=</span> <span onmouseout="hideTip(event, 'fs13', 28)" onmouseover="showTip(event, 'fs13', 28)" class="f">g1</span> [|<span onmouseout="hideTip(event, 'fs8', 29)" onmouseover="showTip(event, 'fs8', 29)" class="p">D</span> <span class="n">1.</span>; <span onmouseout="hideTip(event, 'fs8', 30)" onmouseover="showTip(event, 'fs8', 30)" class="p">D</span> <span class="n">2.</span>|]

<span class="c">// Jacobian of a vector-to-vector function</span>
<span class="c">// j1: D[] -&gt; D[,]</span>
<span class="c">// i.e., take the function arguments as a D[] and return the Jacobian as a D[,]</span>
<span class="c">// Inner lambda expression: D[] -&gt; D[]</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs17', 31)" onmouseover="showTip(event, 'fs17', 31)" class="f">j1</span> <span class="o">=</span> <span onmouseout="hideTip(event, 'fs18', 32)" onmouseover="showTip(event, 'fs18', 32)" class="f">jacobian</span> (<span class="k">fun</span> <span onmouseout="hideTip(event, 'fs15', 33)" onmouseover="showTip(event, 'fs15', 33)" class="i">x</span> <span class="k">-&gt;</span> [|<span onmouseout="hideTip(event, 'fs6', 34)" onmouseover="showTip(event, 'fs6', 34)" class="f">sin</span> <span onmouseout="hideTip(event, 'fs15', 35)" onmouseover="showTip(event, 'fs15', 35)" class="i">x</span><span class="o">.</span>[<span class="n">0</span>]; <span onmouseout="hideTip(event, 'fs19', 36)" onmouseover="showTip(event, 'fs19', 36)" class="f">cos</span> <span onmouseout="hideTip(event, 'fs15', 37)" onmouseover="showTip(event, 'fs15', 37)" class="i">x</span><span class="o">.</span>[<span class="n">1</span>]|])

<span class="c">// Compute the Jacobian at (1, 2)</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs20', 38)" onmouseover="showTip(event, 'fs20', 38)" class="i">j1val</span> <span class="o">=</span> <span onmouseout="hideTip(event, 'fs17', 39)" onmouseover="showTip(event, 'fs17', 39)" class="f">j1</span> [|<span onmouseout="hideTip(event, 'fs8', 40)" onmouseover="showTip(event, 'fs8', 40)" class="p">D</span> <span class="n">1.</span>; <span onmouseout="hideTip(event, 'fs8', 41)" onmouseover="showTip(event, 'fs8', 41)" class="p">D</span> <span class="n">2.</span>|]
</pre>
</td>
</tr>
</table>

<table class="pre"><tr><td><pre><code>val g1 : (D [] -&gt; D [])
val g1val : D [] = [|D -0.8322936731; D -0.4161468365|]
val j1 : (D [] -&gt; D [,])
val j1val : D [,] = [[D 0.5403023059; D 0.0]
                     [D 0.0; D -0.9092974268]]</code></pre></td></tr></table>

<p>In addition to this, every module provides a <strong>Vector</strong> submodule containing versions of the same differentiation operators using the <strong>Vector</strong> and <strong>Matrix</strong> types instead of <strong>float[]</strong> and <strong>float[,]</strong>. This is advantageous in situations where you have to manipulate vectors in the rest of your algorithm. For instance, see the example on <a href="examples-gradientdescent.html">Gradient Descent</a>.</p>

<p>Vector and Matrix operations are handled through the <a href="https://gbaydin.github.io/FsAlg/">FsAlg Linear Algebra Library</a>, which supports most of the commonly used linear algebra operations, including matrix–vector operations, matrix inverse, determinants, eigenvalues, LU and QR decompositions. Please see the API reference of FsAlg for a complete list of supported linear algebra operations.</p>

<table class="pre"><tr><td class="lines"><pre class="fssnip">
<span class="l"> 1: </span>
<span class="l"> 2: </span>
<span class="l"> 3: </span>
<span class="l"> 4: </span>
<span class="l"> 5: </span>
<span class="l"> 6: </span>
<span class="l"> 7: </span>
<span class="l"> 8: </span>
<span class="l"> 9: </span>
<span class="l">10: </span>
<span class="l">11: </span>
<span class="l">12: </span>
<span class="l">13: </span>
<span class="l">14: </span>
<span class="l">15: </span>
<span class="l">16: </span>
<span class="l">17: </span>
<span class="l">18: </span>
<span class="l">19: </span>
<span class="l">20: </span>
<span class="l">21: </span>
<span class="l">22: </span>
<span class="l">23: </span>
<span class="l">24: </span>
<span class="l">25: </span>
<span class="l">26: </span>
<span class="l">27: </span>
<span class="l">28: </span>
<span class="l">29: </span>
<span class="l">30: </span>
<span class="l">31: </span>
</pre>
</td>
<td class="snippet"><pre class="fssnip">
<span class="k">open</span> <span onmouseout="hideTip(event, 'fs1', 43)" onmouseover="showTip(event, 'fs1', 43)" class="i">DiffSharp</span><span class="o">.</span><span onmouseout="hideTip(event, 'fs2', 44)" onmouseover="showTip(event, 'fs2', 44)" class="i">AD</span>
<span class="k">open</span> <span onmouseout="hideTip(event, 'fs1', 45)" onmouseover="showTip(event, 'fs1', 45)" class="i">DiffSharp</span><span class="o">.</span><span onmouseout="hideTip(event, 'fs2', 46)" onmouseover="showTip(event, 'fs2', 46)" class="i">AD</span><span class="o">.</span><span onmouseout="hideTip(event, 'fs21', 47)" onmouseover="showTip(event, 'fs21', 47)" class="i">Vector</span>
<span class="k">open</span> <span onmouseout="hideTip(event, 'fs22', 48)" onmouseover="showTip(event, 'fs22', 48)" class="i">FsAlg</span><span class="o">.</span><span onmouseout="hideTip(event, 'fs23', 49)" onmouseover="showTip(event, 'fs23', 49)" class="i">Generic</span>

<span class="c">// Gradient of a vector-to-scalar function</span>
<span class="c">// g2: Vector&lt;D&gt; -&gt; Vector&lt;D&gt;</span>
<span class="c">// i.e., take the function arguments as a Vector&lt;D&gt; and return the gradient as a Vector&lt;D&gt;</span>
<span class="c">// Inner lambda expression: Vector&lt;D&gt; -&gt; D</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs24', 50)" onmouseover="showTip(event, 'fs24', 50)" class="f">g2</span> <span class="o">=</span> <span onmouseout="hideTip(event, 'fs25', 51)" onmouseover="showTip(event, 'fs25', 51)" class="f">grad</span> (<span class="k">fun</span> <span onmouseout="hideTip(event, 'fs26', 52)" onmouseover="showTip(event, 'fs26', 52)" class="i">x</span> <span class="k">-&gt;</span> <span onmouseout="hideTip(event, 'fs6', 53)" onmouseover="showTip(event, 'fs6', 53)" class="f">sin</span> (<span onmouseout="hideTip(event, 'fs26', 54)" onmouseover="showTip(event, 'fs26', 54)" class="i">x</span><span class="o">.</span>[<span class="n">0</span>] <span class="o">*</span> <span onmouseout="hideTip(event, 'fs26', 55)" onmouseover="showTip(event, 'fs26', 55)" class="i">x</span><span class="o">.</span>[<span class="n">1</span>]))

<span class="c">// Compute the gradient at (1, 2)</span>
<span class="c">// g2val: Vector&lt;D&gt;</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs27', 56)" onmouseover="showTip(event, 'fs27', 56)" class="i">g2val</span> <span class="o">=</span> <span onmouseout="hideTip(event, 'fs24', 57)" onmouseover="showTip(event, 'fs24', 57)" class="f">g2</span> (<span onmouseout="hideTip(event, 'fs28', 58)" onmouseover="showTip(event, 'fs28', 58)" class="f">vector</span> [<span onmouseout="hideTip(event, 'fs8', 59)" onmouseover="showTip(event, 'fs8', 59)" class="p">D</span> <span class="n">1.</span>; <span onmouseout="hideTip(event, 'fs8', 60)" onmouseover="showTip(event, 'fs8', 60)" class="p">D</span> <span class="n">2.</span>])

<span class="c">// Compute the negative of the gradient vector</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs29', 61)" onmouseover="showTip(event, 'fs29', 61)" class="i">g2valb</span> <span class="o">=</span> <span class="o">-</span><span onmouseout="hideTip(event, 'fs27', 62)" onmouseover="showTip(event, 'fs27', 62)" class="i">g2val</span>

<span class="c">// Scale the gradient vector</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs30', 63)" onmouseover="showTip(event, 'fs30', 63)" class="i">g2valc</span> <span class="o">=</span> <span onmouseout="hideTip(event, 'fs8', 64)" onmouseover="showTip(event, 'fs8', 64)" class="p">D</span> <span class="n">0.2</span> <span class="o">*</span> <span onmouseout="hideTip(event, 'fs27', 65)" onmouseover="showTip(event, 'fs27', 65)" class="i">g2val</span>

<span class="c">// Get the norm of the gradient vector</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs31', 66)" onmouseover="showTip(event, 'fs31', 66)" class="i">g2vald</span> <span class="o">=</span> <span onmouseout="hideTip(event, 'fs32', 67)" onmouseover="showTip(event, 'fs32', 67)" class="t">Vector</span><span class="o">.</span><span onmouseout="hideTip(event, 'fs33', 68)" onmouseover="showTip(event, 'fs33', 68)" class="f">norm</span> <span onmouseout="hideTip(event, 'fs27', 69)" onmouseover="showTip(event, 'fs27', 69)" class="i">g2val</span>

<span class="c">// Jacobian of a vector-to-vector function</span>
<span class="c">// j2: Vector&lt;D&gt; -&gt; Matrix&lt;D&gt;</span>
<span class="c">// i.e., take the function arguments as a Vector&lt;D&gt; and return the Jacobian as a Matrix&lt;D&gt;</span>
<span class="c">// Inner lambda expression: Vector&lt;D&gt; -&gt; Vector&lt;D&gt;</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs34', 70)" onmouseover="showTip(event, 'fs34', 70)" class="f">j2</span> <span class="o">=</span> <span onmouseout="hideTip(event, 'fs35', 71)" onmouseover="showTip(event, 'fs35', 71)" class="f">jacobian</span> (<span class="k">fun</span> <span onmouseout="hideTip(event, 'fs26', 72)" onmouseover="showTip(event, 'fs26', 72)" class="i">x</span> <span class="k">-&gt;</span> <span onmouseout="hideTip(event, 'fs28', 73)" onmouseover="showTip(event, 'fs28', 73)" class="f">vector</span> [<span onmouseout="hideTip(event, 'fs6', 74)" onmouseover="showTip(event, 'fs6', 74)" class="f">sin</span> <span onmouseout="hideTip(event, 'fs26', 75)" onmouseover="showTip(event, 'fs26', 75)" class="i">x</span><span class="o">.</span>[<span class="n">0</span>]; <span onmouseout="hideTip(event, 'fs19', 76)" onmouseover="showTip(event, 'fs19', 76)" class="f">cos</span> <span onmouseout="hideTip(event, 'fs26', 77)" onmouseover="showTip(event, 'fs26', 77)" class="i">x</span><span class="o">.</span>[<span class="n">1</span>]])

<span class="c">// Compute the Jacobian at (1, 2)</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs36', 78)" onmouseover="showTip(event, 'fs36', 78)" class="i">j2val</span> <span class="o">=</span> <span onmouseout="hideTip(event, 'fs34', 79)" onmouseover="showTip(event, 'fs34', 79)" class="f">j2</span> (<span onmouseout="hideTip(event, 'fs28', 80)" onmouseover="showTip(event, 'fs28', 80)" class="f">vector</span> [<span onmouseout="hideTip(event, 'fs8', 81)" onmouseover="showTip(event, 'fs8', 81)" class="p">D</span> <span class="n">1.</span>; <span onmouseout="hideTip(event, 'fs8', 82)" onmouseover="showTip(event, 'fs8', 82)" class="p">D</span> <span class="n">2.</span>])
</pre>
</td>
</tr>
</table>

<table class="pre"><tr><td><pre><code>val g2 : (Vector&lt;D&gt; -&gt; Vector&lt;D&gt;)
val g2val : Vector&lt;D&gt; = Vector [|D -0.8322936731; D -0.4161468365|]
val g2valb : Vector&lt;D&gt; = Vector [|D 0.8322936731; D 0.4161468365|]
val g2valc : Vector&lt;D&gt; = Vector [|D -0.1664587346; D -0.08322936731|]
val g2vald : D = D 0.9305326151
val j2 : (Vector&lt;D&gt; -&gt; Matrix&lt;D&gt;)
val j2val : Matrix&lt;D&gt; = Matrix [[D 0.5403023059; D 0.0]
                                [D 0.0; D -0.9092974268]]</code></pre></td></tr></table>

          <div class="tip" id="fs1">namespace DiffSharp</div>
<div class="tip" id="fs2">namespace DiffSharp.AD</div>
<div class="tip" id="fs3">val a : D<br /><br />Full name: Api-overview.a</div>
<div class="tip" id="fs4">val diff : f:(D -&gt; D) -&gt; x:D -&gt; D<br /><br />Full name: DiffSharp.AD.DiffOps.diff</div>
<div class="tip" id="fs5">val x : D</div>
<div class="tip" id="fs6">val sin : value:&#39;T -&gt; &#39;T (requires member Sin)<br /><br />Full name: Microsoft.FSharp.Core.Operators.sin</div>
<div class="tip" id="fs7">val sqrt : value:&#39;T -&gt; &#39;U (requires member Sqrt)<br /><br />Full name: Microsoft.FSharp.Core.Operators.sqrt</div>
<div class="tip" id="fs8">union case D.D: float -&gt; D</div>
<div class="tip" id="fs9">val b : D<br /><br />Full name: Api-overview.b</div>
<div class="tip" id="fs10">val c : D<br /><br />Full name: Api-overview.c</div>
<div class="tip" id="fs11">val diff&#39; : f:(D -&gt; D) -&gt; x:D -&gt; D * D<br /><br />Full name: DiffSharp.AD.DiffOps.diff&#39;</div>
<div class="tip" id="fs12">val printf : format:Printf.TextWriterFormat&lt;&#39;T&gt; -&gt; &#39;T<br /><br />Full name: Microsoft.FSharp.Core.ExtraTopLevelOperators.printf</div>
<div class="tip" id="fs13">val g1 : (D [] -&gt; D [])<br /><br />Full name: Api-overview.g1</div>
<div class="tip" id="fs14">val grad : f:(D [] -&gt; D) -&gt; x:D [] -&gt; D []<br /><br />Full name: DiffSharp.AD.DiffOps.grad</div>
<div class="tip" id="fs15">val x : D []</div>
<div class="tip" id="fs16">val g1val : D []<br /><br />Full name: Api-overview.g1val</div>
<div class="tip" id="fs17">val j1 : (D [] -&gt; D [,])<br /><br />Full name: Api-overview.j1</div>
<div class="tip" id="fs18">val jacobian : f:(D [] -&gt; D []) -&gt; x:D [] -&gt; D [,]<br /><br />Full name: DiffSharp.AD.DiffOps.jacobian</div>
<div class="tip" id="fs19">val cos : value:&#39;T -&gt; &#39;T (requires member Cos)<br /><br />Full name: Microsoft.FSharp.Core.Operators.cos</div>
<div class="tip" id="fs20">val j1val : D [,]<br /><br />Full name: Api-overview.j1val</div>
<div class="tip" id="fs21">module Vector<br /><br />from DiffSharp.AD</div>
<div class="tip" id="fs22">namespace FsAlg</div>
<div class="tip" id="fs23">namespace FsAlg.Generic</div>
<div class="tip" id="fs24">val g2 : (Vector&lt;D&gt; -&gt; Vector&lt;D&gt;)<br /><br />Full name: Api-overview.g2</div>
<div class="tip" id="fs25">val grad : f:(Vector&lt;D&gt; -&gt; D) -&gt; x:Vector&lt;D&gt; -&gt; Vector&lt;D&gt;<br /><br />Full name: DiffSharp.AD.Vector.grad</div>
<div class="tip" id="fs26">val x : Vector&lt;D&gt;</div>
<div class="tip" id="fs27">val g2val : Vector&lt;D&gt;<br /><br />Full name: Api-overview.g2val</div>
<div class="tip" id="fs28">val vector : v:seq&lt;&#39;T&gt; -&gt; Vector&lt;&#39;T&gt; (requires member get_Zero and member get_One and member ( + ) and member ( - ) and member ( * ) and member ( / ) and member ( ~- ) and member Abs and member Pow and member Sqrt and member op_Explicit and comparison)<br /><br />Full name: FsAlg.Generic.Ops.vector</div>
<div class="tip" id="fs29">val g2valb : Vector&lt;D&gt;<br /><br />Full name: Api-overview.g2valb</div>
<div class="tip" id="fs30">val g2valc : Vector&lt;D&gt;<br /><br />Full name: Api-overview.g2valc</div>
<div class="tip" id="fs31">val g2vald : D<br /><br />Full name: Api-overview.g2vald</div>
<div class="tip" id="fs32">Multiple items<br />union case Vector.Vector: &#39;T [] -&gt; Vector&lt;&#39;T&gt;<br /><br />--------------------<br />module Vector<br /><br />from FsAlg.Generic<br /><br />--------------------<br />module Vector<br /><br />from DiffSharp.AD<br /><br />--------------------<br />type Vector&lt;&#39;T (requires member get_Zero and member get_One and member ( + ) and member ( - ) and member ( * ) and member ( / ) and member ( ~- ) and member Abs and member Pow and member Sqrt and member op_Explicit and comparison)&gt; =<br />&#160;&#160;| ZeroVector of &#39;T<br />&#160;&#160;| Vector of &#39;T []<br />&#160;&#160;member Convert : f:(&#39;T -&gt; &#39;a) -&gt; Vector&lt;&#39;a&gt; (requires member get_Zero and member get_One and member ( + ) and member ( - ) and member ( * ) and member ( / ) and member ( ~- ) and member Abs and member Pow and member Sqrt and member op_Explicit and comparison)<br />&#160;&#160;member Copy : unit -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;member GetEnumerator : unit -&gt; IEnumerator&lt;&#39;T&gt;<br />&#160;&#160;member GetL1Norm : unit -&gt; &#39;T<br />&#160;&#160;member GetL2Norm : unit -&gt; &#39;T<br />&#160;&#160;member GetL2NormSq : unit -&gt; &#39;T<br />&#160;&#160;member GetLPNorm : p:&#39;T -&gt; &#39;T<br />&#160;&#160;member GetMax : unit -&gt; &#39;T<br />&#160;&#160;member GetMaxBy : f:(&#39;T -&gt; &#39;a1) -&gt; &#39;T (requires comparison)<br />&#160;&#160;member GetMin : unit -&gt; &#39;T<br />&#160;&#160;member GetMinBy : f:(&#39;T -&gt; &#39;a1) -&gt; &#39;T (requires comparison)<br />&#160;&#160;member GetSlice : lower:int option * upper:int option -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;member GetSubVector : s:int * c:int -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;member GetUnitVector : unit -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;member Split : n:seq&lt;int&gt; -&gt; seq&lt;Vector&lt;&#39;T&gt;&gt;<br />&#160;&#160;member SplitEqual : n:int -&gt; seq&lt;Vector&lt;&#39;T&gt;&gt;<br />&#160;&#160;member ToArray : unit -&gt; &#39;T []<br />&#160;&#160;member ToMathematicaString : unit -&gt; string<br />&#160;&#160;member ToMatlabString : unit -&gt; string<br />&#160;&#160;member ToSeq : unit -&gt; seq&lt;&#39;T&gt;<br />&#160;&#160;member FirstItem : &#39;T<br />&#160;&#160;member Item : i:int -&gt; &#39;T with get<br />&#160;&#160;member Length : int<br />&#160;&#160;member Item : i:int -&gt; &#39;T with set<br />&#160;&#160;static member Zero : Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( + ) : a:&#39;T * b:Vector&lt;&#39;T&gt; -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( + ) : a:Vector&lt;&#39;T&gt; * b:&#39;T -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( + ) : a:Vector&lt;&#39;T&gt; * b:Vector&lt;&#39;T&gt; -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( / ) : a:&#39;T * b:Vector&lt;&#39;T&gt; -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( / ) : a:Vector&lt;&#39;T&gt; * b:&#39;T -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( ./ ) : a:Vector&lt;&#39;T&gt; * b:Vector&lt;&#39;T&gt; -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( .* ) : a:Vector&lt;&#39;T&gt; * b:Vector&lt;&#39;T&gt; -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member op_Explicit : v:Vector&lt;&#39;T&gt; -&gt; float []<br />&#160;&#160;static member ( * ) : a:&#39;T * b:Vector&lt;&#39;T&gt; -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( * ) : a:Vector&lt;&#39;T&gt; * b:&#39;T -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( * ) : a:Vector&lt;&#39;T&gt; * b:Vector&lt;&#39;T&gt; -&gt; &#39;T<br />&#160;&#160;static member ( %* ) : a:Vector&lt;&#39;T&gt; * b:Vector&lt;&#39;T&gt; -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( - ) : a:&#39;T * b:Vector&lt;&#39;T&gt; -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( - ) : a:Vector&lt;&#39;T&gt; * b:&#39;T -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( - ) : a:Vector&lt;&#39;T&gt; * b:Vector&lt;&#39;T&gt; -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( ~- ) : a:Vector&lt;&#39;T&gt; -&gt; Vector&lt;&#39;T&gt;<br /><br />Full name: FsAlg.Generic.Vector&lt;_&gt;</div>
<div class="tip" id="fs33">val norm : v:Vector&lt;&#39;T&gt; -&gt; &#39;T (requires member get_Zero and member get_One and member ( + ) and member ( - ) and member ( * ) and member ( / ) and member ( ~- ) and member Abs and member Pow and member Sqrt and member op_Explicit and comparison)<br /><br />Full name: FsAlg.Generic.Vector.norm</div>
<div class="tip" id="fs34">val j2 : (Vector&lt;D&gt; -&gt; Matrix&lt;D&gt;)<br /><br />Full name: Api-overview.j2</div>
<div class="tip" id="fs35">val jacobian : f:(Vector&lt;D&gt; -&gt; Vector&lt;D&gt;) -&gt; x:Vector&lt;D&gt; -&gt; Matrix&lt;D&gt;<br /><br />Full name: DiffSharp.AD.Vector.jacobian</div>
<div class="tip" id="fs36">val j2val : Matrix&lt;D&gt;<br /><br />Full name: Api-overview.j2val</div>
          
        </div>
        <div class="span3">
          <a href="index.html"><img src="img/diffsharp-logo.png" style="width:140px;height:140px;margin:10px 0px 0px 20px;border-style:none;"/></a>

          <ul class="nav nav-list" id="menu">
            <li class="nav-header">DiffSharp</li>
            <li class="divider"></li>
            <li><a href="index.html">Home Page</a></li>
            <li><a href="https://www.nuget.org/packages/diffsharp">Get DiffSharp via NuGet</a></li>
            <li><a href="https://github.com/DiffSharp/DiffSharp">GitHub Page</a></li>
            <li><a href="https://github.com/DiffSharp/DiffSharp/releases">Release Notes</a></li>

            <li class="nav-header">Getting Started</li>
            <li class="divider"></li>
            <li><a href="gettingstarted-typeinference.html">Type Inference</a></li>
            <li><a href="api-overview.html">API Overview</a></li>
            <li><a href="gettingstarted-nestedad.html">Nested AD</a></li>
            <li><a href="gettingstarted-nonnestedad.html">Non-nested AD</a></li>
            <li><a href="gettingstarted-numericaldifferentiation.html">Numerical Differentiation</a></li>
            <li><a href="gettingstarted-symbolicdifferentiation.html">Symbolic Differentiation</a></li>
            <li><a href="benchmarks.html">Benchmarks</a></li>
            <li><a href="csharp.html">C# and Other Languages</a></li>            
            <li><a href="reference/index.html">API Reference</a></li>

            <li class="nav-header">Examples</li>
            <li class="divider"></li>

            <li class="nav-header">Machine Learning</li>
            <li><a href="examples-gradientdescent.html">Gradient Descent</a></li>
            <li><a href="examples-newtonsmethod.html">Newton's Method</a></li>
            <li><a href="examples-stochasticgradientdescent.html">Stochastic Gradient Descent</a></li>
            <li><a href="examples-kmeansclustering.html">K-Means Clustering</a></li>
            <li><a href="examples-hamiltonianmontecarlo.html">Hamiltonian Monte Carlo</a></li>
            <li><a href="examples-neuralnetworks.html">Neural Networks</a></li>
            <li>Neural Turing Machines (to come)</li>
            <li>Probabilistic Programming<br>(to come)</li>

            <li class="nav-header">Dynamical Systems</li>
            <li>Stability Analysis (to come)</li>

            <li class="nav-header">Control</li>
            <li><a href="examples-inversekinematics.html">Inverse Kinematics</a></li>
            <li>Adaptive Control (to come)</li>

            <li class="nav-header">Physics</li>
            <li><a href="examples-kinematics.html">Kinematics</a></li>
            <li>Leapfrog Integration (to come)</li>
            <li><a href="examples-helmholtzenergyfunction.html">Helmholtz Energy Function</a></li>

            <li class="nav-header">Math</li>
            <li><a href="examples-lhopitalsrule.html">l'Hôpital's Rule</a></li>

            <li class="nav-header">Makers</li>
            <li class="divider"></li>
            <li><a href="https://www.cs.nuim.ie/~gunes/">Atılım Güneş Baydin</a></li>
            <li><a href="http://www.bcl.hamilton.ie/~barak/">Barak A. Pearlmutter</a></li>
            <li><a href="http://www.bcl.hamilton.ie/">Brain and Computation Lab</a></li>
          </ul>
        </div>
      </div>
    </div>
    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=10059115; 
    var sc_invisible=1; 
    var sc_security="92275ee1"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="https://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="https://c.statcounter.com/10059115/0/92275ee1/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->
  </body>
</html>