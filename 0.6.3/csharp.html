<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <!-- 
      The Interoperability with Other Languages
 parameters will be replaced with the 
      document title extracted from the <h1> element or
      file name, if there is no <h1> heading
    -->
    <title>Interoperability with Other Languages
</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Atılım Güneş Baydin; Barak A. Pearlmutter">
    <meta name="description" content="DiffSharp is an automatic differentiation (AD) library implemented in the F# language by Atılım Güneş Baydin and Barak A. Pearlmutter, mainly for research applications in machine learning, as part of their work at the Brain and Computation Lab, Hamilton Institute, National University of Ireland Maynooth.">

    <script src="https://code.jquery.com/jquery-1.8.0.js"></script>
    <script src="https://code.jquery.com/ui/1.8.23/jquery-ui.js"></script>
    <script src="https://netdna.bootstrapcdn.com/twitter-bootstrap/2.2.1/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link href="https://netdna.bootstrapcdn.com/twitter-bootstrap/2.2.1/css/bootstrap-combined.min.css" rel="stylesheet">
    
    <link type="text/css" rel="stylesheet" href="misc/style.css" />
    <script src="misc/tips.js" type="text/javascript"></script>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-48900508-3', 'auto');
      ga('require', 'displayfeatures');
      ga('send', 'pageview');
    </script>
  </head>
  <body>
    <div class="container">
      <div class="masthead">
        <ul class="nav nav-pills pull-right">
          <li><a href="https://fsharp.org">fsharp.org</a></li>
        </ul>
        <h3 class="muted">DiffSharp</h3>
      </div>
      <hr />
      <div class="row">
        <div class="span9" id="main">
          <h1>Interoperability with Other Languages</h1>

<p>As F# can interoperate seamlessly with C# and other <a href="https://en.wikipedia.org/wiki/List_of_CLI_languages">CLI languages</a>, DiffSharp can be used with these languages as well. Your project should reference the <strong>DiffSharp.dll</strong> assembly, its dependencies, and also the <strong>FSharp.Core.dll</strong> assembly.</p>

<p>For C# and other languages, the <strong>DiffSharp.Interop</strong> namespace provides a simple way of accessing the main functionality. (Without <strong>DiffSharp.Interop</strong>, you can still use the regular DiffSharp namespaces, but you will need to take care of issues such as converting to and from <a href="https://msdn.microsoft.com/en-us/library/ee340302.aspx"><strong>FSharp.Core.FSharpFunc</strong></a> objects.)</p>

<h1>Using DiffSharp with C#</h1>

<h2>Nested Automatic Differentiation</h2>

<p>For using the nested forward and reverse AD capability, you need to write the part of your numeric code where you need deriatives (e.g. for optimization) using the <strong>DiffSharp.Interop.D</strong> numeric type, the results of which you may convert later to a standard type such as <strong>double</strong>. In other words, for any computation you do with the <strong>D</strong> numeric type, you can automatically get exact derivatives.</p>

<p>The <strong>DiffSharp.Interop.AD</strong> class provides common mathematical functions (e.g. <strong>AD.Exp</strong>, <strong>AD.Sin</strong>, <strong>AD.Pow</strong> ) for the <strong>D</strong> type, similar to the use of <a href="https://msdn.microsoft.com/en-us/library/System.Math(v=vs.110).aspx"><strong>System.Math</strong></a> class with the <strong>double</strong> type and other types.</p>

<p>C# versions of the differentiation operations are also provided through the <strong>DiffSharp.Interop.AD</strong> wrapper class, which internally handles all necessary conversions to and from C# functions. The names of differentiation operations (e.g. <strong>diff</strong>, <strong>grad</strong>, <strong>hessian</strong> ) remain the same, but their first letters are capitalized (e.g. <strong>AD.Diff</strong>, <strong>AD.Grad</strong>, <strong>AD.Hessian</strong> ). Please see the <a href="api-overview.html">API Overview</a> page for general information about the differentiation API.</p>

<p>Here is a simple example illustrating the use of the <strong>D</strong> type and the computation of derivatives.</p>

<table class="pre"><tr><td class="lines"><span class="l"> 1: </span>
<span class="l"> 2: </span>
<span class="l"> 3: </span>
<span class="l"> 4: </span>
<span class="l"> 5: </span>
<span class="l"> 6: </span>
<span class="l"> 7: </span>
<span class="l"> 8: </span>
<span class="l"> 9: </span>
<span class="l">10: </span>
<span class="l">11: </span>
<span class="l">12: </span>
<span class="l">13: </span>
<span class="l">14: </span>
<span class="l">15: </span>
<span class="l">16: </span>
<span class="l">17: </span>
<span class="l">18: </span>
<span class="l">19: </span>
<span class="l">20: </span>
<span class="l">21: </span>
<span class="l">22: </span>
<span class="l">23: </span>
<span class="l">24: </span>
<span class="l">25: </span>
<span class="l">26: </span>
<span class="l">27: </span>
<span class="l">28: </span>
<span class="l">29: </span>
<span class="l">30: </span>
<span class="l">31: </span>
<span class="l">32: </span>
<span class="l">33: </span>
<span class="l">34: </span>
<span class="l">35: </span>
<span class="l">36: </span>
<span class="l">37: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Open DiffSharp interop</span>
<span class="k">using</span> DiffSharp.Interop;

<span class="k">class</span> Program
{
    <span class="c">// Define a function whose derivative you need</span>
    <span class="c">// F(x) = Sin(x^2 - Exp(x))</span>
    <span class="k">public</span> <span class="k">static</span> D F(D x)
    {
        <span class="k">return</span> AD.Sin(x <span class="o">*</span> x <span class="o">-</span> AD.Exp(x));
    }

    <span class="k">public</span> <span class="k">static</span> <span class="k">void</span> Main(<span class="k">string</span>[] args)
    {
        <span class="c">// You can compute the value of the derivative of F at a point</span>
        D da <span class="o">=</span> AD.Diff(F, 2.3);

        <span class="c">// Or, you can generate a derivative function which you may use for many evaluations</span>
        <span class="c">// dF is the derivative function of F</span>
        <span class="k">var</span> dF <span class="o">=</span> AD.Diff(F);

        <span class="c">// Evaluate the derivative function at different points</span>
        D db <span class="o">=</span> dF(2.3);
        D dc <span class="o">=</span> dF(1.4);

        <span class="c">// Creation and conversion of D values</span>

        <span class="c">// Construct new D</span>
        D a <span class="o">=</span> <span class="k">new</span> D(4.1);

        <span class="c">// Cast double to D</span>
        D b <span class="o">=</span> (D)4.1;

        <span class="c">// Cast D to double</span>
        <span class="k">double</span> c <span class="o">=</span> (<span class="k">double</span>)a;
    }
}
</pre></td></tr></table>

<p>Differentiation operations can be nested, meaning that you can compute higher-order derivatives and differentiate functions that are themselves internally making use of differentiation (also see <a href="gettingstarted-nestedad.html">Nested AD</a>).</p>

<table class="pre"><tr><td class="lines"><span class="l"> 1: </span>
<span class="l"> 2: </span>
<span class="l"> 3: </span>
<span class="l"> 4: </span>
<span class="l"> 5: </span>
<span class="l"> 6: </span>
<span class="l"> 7: </span>
<span class="l"> 8: </span>
<span class="l"> 9: </span>
<span class="l">10: </span>
<span class="l">11: </span>
<span class="l">12: </span>
<span class="l">13: </span>
<span class="l">14: </span>
<span class="l">15: </span>
<span class="l">16: </span>
<span class="l">17: </span>
<span class="l">18: </span>
<span class="l">19: </span>
<span class="l">20: </span>
<span class="l">21: </span>
<span class="l">22: </span>
<span class="l">23: </span>
<span class="l">24: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="k">using</span> DiffSharp.Interop;

<span class="k">class</span> Program
{
    <span class="c">// F(x) = Sin(x^2 - Exp(x))</span>
    <span class="k">public</span> D F(D x)
    {
        <span class="k">return</span> AD.Sin(x <span class="o">*</span> x <span class="o">-</span> AD.Exp(x));
    }

    <span class="c">// G is internally using the derivative of F</span>
    <span class="c">// G(x) = F'(x) / Exp(x^3)</span>
    <span class="k">public</span> D G(D x)
    {
        <span class="k">return</span> AD.Diff(F, x) <span class="o">/</span> AD.Exp(AD.Pow(x, 3));
    }

    <span class="c">// H is internally using the derivative of G</span>
    <span class="c">// H(x) = Sin(G'(x) / 2)</span>
    <span class="k">public</span> D H(D x)
    {
        <span class="k">return</span> AD.Sin(AD.Diff(G, x) <span class="o">/</span> 2);
    }
}
</pre></td></tr></table>

<p>A convenient way of writing functions is to use <a href="https://msdn.microsoft.com/en-us/library/bb397687.aspx">C# lambda expressions</a> with which you can define local anonymous functions.</p>

<table class="pre"><tr><td class="lines"><span class="l"> 1: </span>
<span class="l"> 2: </span>
<span class="l"> 3: </span>
<span class="l"> 4: </span>
<span class="l"> 5: </span>
<span class="l"> 6: </span>
<span class="l"> 7: </span>
<span class="l"> 8: </span>
<span class="l"> 9: </span>
<span class="l">10: </span>
<span class="l">11: </span>
<span class="l">12: </span>
<span class="l">13: </span>
<span class="l">14: </span>
<span class="l">15: </span>
<span class="l">16: </span>
<span class="l">17: </span>
<span class="l">18: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="k">using</span> DiffSharp.Interop;

<span class="k">class</span> Program
{
    <span class="c">// F(x) = Sin(x^2 - Exp(x))</span>
    <span class="k">public</span> <span class="k">static</span> D F(D x)
    {
        <span class="k">return</span> AD.Sin(x <span class="o">*</span> x <span class="o">-</span> AD.Exp(x));
    }

    <span class="k">public</span> <span class="k">static</span> <span class="k">void</span> Main(<span class="k">string</span>[] args)
    {
        <span class="c">// Derivative of F(x) at x = 3</span>
        <span class="k">var</span> a <span class="o">=</span> AD.Diff(F, 3);

        <span class="c">// This is the same with above, defining the function inline</span>
        <span class="k">var</span> b <span class="o">=</span> AD.Diff(x <span class="o">=</span><span class="o">&gt;</span> AD.Sin(x <span class="o">*</span> x <span class="o">-</span> AD.Exp(x)), 3);
    }
</pre></td></tr></table>

<p>DiffSharp can handle nested cases such as computing the derivative of a function <span class="math">\(f\)</span> that takes an argument <span class="math">\(x\)</span>, which, in turn, computes the derivative of another function <span class="math">\(g\)</span> nested inside <span class="math">\(f\)</span> that has a free reference to <span class="math">\(x\)</span>, the argument to the surrounding function.</p>

<p><span class="math">\[  \frac{d}{dx} \left. \left( x \left( \left. \frac{d}{dy} x y \; \right|_{y=3} \right) \right) \right|_{x=2}\]</span></p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="k">var</span> c <span class="o">=</span> AD.Diff(x <span class="o">=</span><span class="o">&gt;</span> x <span class="o">*</span> AD.Diff(y <span class="o">=</span><span class="o">&gt;</span> x <span class="o">*</span> y, 3), 2);
</pre></td></tr></table>

<p>This allows you to write, for example, nested optimization algorithms of the form</p>

<p><span class="math">\[  \mathbf{min} \left( \lambda x \; . \; (f \; x) + \mathbf{min} \left( \lambda y \; . \; g \; x \; y \right) \right)\; ,\]</span></p>

<p>for functions <span class="math">\(f\)</span> and <span class="math">\(g\)</span> and a gradient-based minimization procedure <span class="math">\(\mathbf{min}\)</span>.</p>

<h3>Differentiation Operations</h3>

<p>Currently the following operations are supported by <strong>DiffSharp.Interop.AD</strong>:</p>

<h3>AD.Diff</h3>

<h4>First derivative of a scalar-to-scalar function</h4>

<p>Syntax: <code>public static Func&lt;D,D&gt; AD.Diff(Func&lt;D,D&gt; f)</code></p>

<p>For a function <span class="math">\(f(a): \mathbb{R} \to \mathbb{R}\)</span>, this returns a function that computes the derivative</p>

<p><span class="math">\[  \frac{d}{da} f(a) \; .\]</span></p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
<span class="l">2: </span>
<span class="l">3: </span>
<span class="l">4: </span>
<span class="l">5: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Derivative of a scalar-to-scalar function</span>
<span class="k">var</span> df <span class="o">=</span> AD.Diff(x <span class="o">=</span><span class="o">&gt;</span> AD.Sin(x <span class="o">*</span> x <span class="o">-</span> AD.Exp(x)));

<span class="c">// Evaluate df at a point</span>
<span class="k">var</span> v <span class="o">=</span> df(3);
</pre></td></tr></table>

<h4>First derivative of a scalar-to-scalar function evaluated at a point</h4>

<p>Syntax: <code>public static D AD.Diff(Func&lt;D,D&gt; f, D x)</code></p>

<p>For a function <span class="math">\(f(a): \mathbb{R} \to \mathbb{R}\)</span>, and <span class="math">\(x \in \mathbb{R}\)</span>, this returns the derivative evaluated at <span class="math">\(x\)</span></p>

<p><span class="math">\[  \left. \frac{d}{da} f(a) \right|_{a\; =\; x} \; .\]</span></p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
<span class="l">2: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Derivative of a scalar-to-scalar function at a point</span>
<span class="k">var</span> v <span class="o">=</span> AD.Diff(x <span class="o">=</span><span class="o">&gt;</span> AD.Sin(x <span class="o">*</span> x <span class="o">-</span> AD.Exp(x)), 3);
</pre></td></tr></table>

<h3>AD.Diff2</h3>

<h4>Second derivative of a scalar-to-scalar function</h4>

<p>Syntax: <code>public static Func&lt;D,D&gt; AD.Diff2(Func&lt;D,D&gt; f)</code></p>

<p>For a function <span class="math">\(f(a): \mathbb{R} \to \mathbb{R}\)</span>, this returns a function that computes the second derivative</p>

<p><span class="math">\[  \frac{d^2}{da^2} f(a) \; .\]</span></p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
<span class="l">2: </span>
<span class="l">3: </span>
<span class="l">4: </span>
<span class="l">5: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Second derivative of a scalar-to-scalar function</span>
<span class="k">var</span> df <span class="o">=</span> AD.Diff2(x <span class="o">=</span><span class="o">&gt;</span> AD.Sin(x <span class="o">*</span> x <span class="o">-</span> AD.Exp(x)));

<span class="c">// Evaluate df at a point</span>
<span class="k">var</span> v <span class="o">=</span> df(3);
</pre></td></tr></table>

<h4>Second derivative of a scalar-to-scalar function evaluated at a point</h4>

<p>Syntax: <code>public static D AD.Diff2(Func&lt;D,D&gt; f, D x)</code></p>

<p>For a function <span class="math">\(f(a): \mathbb{R} \to \mathbb{R}\)</span>, and <span class="math">\(x \in \mathbb{R}\)</span>, this returns the second derivative evaluated at <span class="math">\(x\)</span></p>

<p><span class="math">\[  \left. \frac{d^2}{da^2} f(a) \right|_{a\; =\; x} \; .\]</span></p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
<span class="l">2: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Second derivative of a scalar-to-scalar function at a point</span>
<span class="k">var</span> v <span class="o">=</span> AD.Diff2(x <span class="o">=</span><span class="o">&gt;</span> AD.Sin(x <span class="o">*</span> x <span class="o">-</span> AD.Exp(x)), 3);
</pre></td></tr></table>

<h3>AD.Diffn</h3>

<h4>N-th derivative of a scalar-to-scalar function</h4>

<p>Syntax: <code>public static Func&lt;D,D&gt; AD.Diffn(Int32 n, Func&lt;D,D&gt; f)</code></p>

<p>For <span class="math">\(n \in \mathbb{N}\)</span> and a function <span class="math">\(f(a): \mathbb{R} \to \mathbb{R}\)</span>, this returns a function that computes the n-th derivative</p>

<p><span class="math">\[  \frac{d^n}{da^n} f(a) \; .\]</span></p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
<span class="l">2: </span>
<span class="l">3: </span>
<span class="l">4: </span>
<span class="l">5: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Fifth derivative of a scalar-to-scalar function</span>
<span class="k">var</span> df <span class="o">=</span> AD.Diffn(5, x <span class="o">=</span><span class="o">&gt;</span> AD.Sin(x <span class="o">*</span> x <span class="o">-</span> AD.Exp(x)));

<span class="c">// Evaluate df at a point</span>
<span class="k">var</span> v <span class="o">=</span> df(3);
</pre></td></tr></table>

<h4>N-th derivative of a scalar-to-scalar function evaluated at a point</h4>

<p>Syntax: <code>public static D AD.Diffn(Int32 n, Func&lt;D,D&gt; f, D x)</code></p>

<p>For <span class="math">\(n \in \mathbb{N}\)</span>, a function <span class="math">\(f(a): \mathbb{R} \to \mathbb{R}\)</span>, and <span class="math">\(x \in \mathbb{R}\)</span>, this returns the n-th derivative evaluated at <span class="math">\(x\)</span></p>

<p><span class="math">\[  \left. \frac{d^n}{da^n} f(a) \right|_{a\; =\; x} \; .\]</span></p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
<span class="l">2: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Fifth derivative of a scalar-to-scalar function at a point</span>
<span class="k">var</span> v <span class="o">=</span> AD.Diffn(5, x <span class="o">=</span><span class="o">&gt;</span> AD.Sin(x <span class="o">*</span> x <span class="o">-</span> AD.Exp(x)), 3);
</pre></td></tr></table>

<h3>AD.Grad</h3>

<h4>Gradient of a vector-to-scalar function</h4>

<p>Syntax: <code>public static Func&lt;D[],D[]&gt; AD.Grad(Func&lt;D[], D&gt; f)</code></p>

<p>For a function <span class="math">\(f(a_1, \dots, a_n): \mathbb{R}^n \to \mathbb{R}\)</span>, this returns a function that computes the <a href="https://en.wikipedia.org/wiki/Gradient">gradient</a></p>

<p><span class="math">\[  \nabla f = \left[ \frac{\partial f}{{\partial a}_1}, \dots, \frac{\partial f}{{\partial a}_n} \right] \; .\]</span></p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
<span class="l">2: </span>
<span class="l">3: </span>
<span class="l">4: </span>
<span class="l">5: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Gradient of a vector-to-scalar function</span>
<span class="k">var</span> gf <span class="o">=</span> AD.Grad(x <span class="o">=</span><span class="o">&gt;</span> AD.Sin(x[0] <span class="o">*</span> x[1]));

<span class="c">// Evaluate gf at a point</span>
<span class="k">var</span> v <span class="o">=</span> gf(<span class="k">new</span> D[] { 3, 2 });
</pre></td></tr></table>

<h4>Gradient of a vector-to-scalar function evaluated at a point</h4>

<p>Syntax: <code>public static D[] AD.Grad(Func&lt;D[],D&gt; f, D[] x)</code></p>

<p>For a function <span class="math">\(f(a_1, \dots, a_n): \mathbb{R}^n \to \mathbb{R}\)</span>, and <span class="math">\(\mathbf{x} \in \mathbb{R}^n\)</span>, this returns the gradient evaluated at <span class="math">\(\mathbf{x}\)</span></p>

<p><span class="math">\[  \left( \nabla f \right)_\mathbf{x} = \left. \left[ \frac{\partial f}{{\partial a}_1}, \dots, \frac{\partial f}{{\partial a}_n} \right] \right|_{\mathbf{a}\; = \; \mathbf{x}} \; .\]</span></p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
<span class="l">2: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Gradient of a vector-to-scalar function at a point</span>
<span class="k">var</span> v <span class="o">=</span> AD.Grad(x <span class="o">=</span><span class="o">&gt;</span> AD.Sin(x[0] <span class="o">*</span> x[1]), <span class="k">new</span> D[] { 3, 2 });
</pre></td></tr></table>

<h3>AD.Gradv</h3>

<h4>Gradient-vector product (directional derivative)</h4>

<p>Syntax: <code>public static D AD.Gradv(Func&lt;D[],D&gt; f, D[] x, D[] v)</code></p>

<p>For a function <span class="math">\(f: \mathbb{R}^n \to \mathbb{R}\)</span>, and <span class="math">\(\mathbf{x}, \mathbf{v} \in \mathbb{R}^n\)</span>, this returns the <a href="https://en.wikipedia.org/wiki/Directional_derivative">gradient-vector product</a> (directional derivative), that is, the dot product of the gradient of <span class="math">\(f\)</span> at <span class="math">\(\mathbf{x}\)</span> with <span class="math">\(\mathbf{v}\)</span></p>

<p><span class="math">\[  \left( \nabla f \right)_\mathbf{x} \cdot \mathbf{v} \; .\]</span></p>

<p>With AD, this value is computed efficiently in one forward evaluation of the function, without computing the full gradient.</p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
<span class="l">2: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Gradient-vector product of a vector-to-scalar function</span>
<span class="k">var</span> v <span class="o">=</span> AD.Gradv(x <span class="o">=</span><span class="o">&gt;</span> AD.Sin(x[0] <span class="o">*</span> x[1]), <span class="k">new</span> D[] { 3, 2 }, <span class="k">new</span> D[] { 5, 3 } );
</pre></td></tr></table>

<h3>AD.Hessian</h3>

<h4>Hessian of a vector-to-scalar function</h4>

<p>Syntax: <code>public static Func&lt;D[],D[,]&gt; AD.Hessian(Func&lt;D[],D&gt; f)</code></p>

<p>For a function <span class="math">\(f(a_1, \dots, a_n): \mathbb{R}^n \to \mathbb{R}\)</span>, this returns a function that computes the <a href="https://en.wikipedia.org/wiki/Hessian_matrix">Hessian matrix</a></p>

<p><span class="math">\[  \mathbf{H}_f = \begin{bmatrix}
                    \frac{\partial ^2 f}{\partial a_1^2} &amp; \frac{\partial ^2 f}{\partial a_1 \partial a_2} &amp; \cdots &amp; \frac{\partial ^2 f}{\partial a_1 \partial a_n} \\
                    \frac{\partial ^2 f}{\partial a_2 \partial a_1} &amp; \frac{\partial ^2 f}{\partial a_2^2} &amp; \cdots &amp; \frac{\partial ^2 f}{\partial a_2 \partial a_n} \\
                    \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
                    \frac{\partial ^2 f}{\partial a_n \partial a_1} &amp; \frac{\partial ^2 f}{\partial a_n \partial a_2} &amp; \cdots &amp; \frac{\partial ^2 f}{\partial a_n^2}
                    \end{bmatrix} \; .\]</span></p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
<span class="l">2: </span>
<span class="l">3: </span>
<span class="l">4: </span>
<span class="l">5: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Hessian of a vector-to-scalar function</span>
<span class="k">var</span> hf <span class="o">=</span> AD.Hessian(x <span class="o">=</span><span class="o">&gt;</span> AD.Sin(x[0] <span class="o">*</span> x[1]));

<span class="c">// Evaluate hf at a point</span>
<span class="k">var</span> v <span class="o">=</span> hf(<span class="k">new</span> D[] { 3, 2 });
</pre></td></tr></table>

<h4>Hessian of a vector-to-scalar function evaluated at a point</h4>

<p>Syntax: <code>public static D[,] AD.Hessian(Func&lt;D[],D&gt; f, D[] x)</code></p>

<p>For a function <span class="math">\(f(a_1, \dots, a_n): \mathbb{R}^n \to \mathbb{R}\)</span>, and <span class="math">\(\mathbf{x} \in \mathbb{R}^n\)</span>, this returns the Hessian matrix evaluated at <span class="math">\(\mathbf{x}\)</span></p>

<p><span class="math">\[  \left( \mathbf{H}_f \right)_\mathbf{x} = \left. \begin{bmatrix}
                                           \frac{\partial ^2 f}{\partial a_1^2} &amp; \frac{\partial ^2 f}{\partial a_1 \partial a_2} &amp; \cdots &amp; \frac{\partial ^2 f}{\partial a_1 \partial a_n} \\
                                           \frac{\partial ^2 f}{\partial a_2 \partial a_1} &amp; \frac{\partial ^2 f}{\partial a_2^2} &amp; \cdots &amp; \frac{\partial ^2 f}{\partial a_2 \partial a_n} \\
                                           \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
                                           \frac{\partial ^2 f}{\partial a_n \partial a_1} &amp; \frac{\partial ^2 f}{\partial a_n \partial a_2} &amp; \cdots &amp; \frac{\partial ^2 f}{\partial a_n^2}
                                          \end{bmatrix} \right|_{\mathbf{a}\; = \; \mathbf{x}} \; .\]</span></p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
<span class="l">2: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Hessian of a vector-to-scalar function at a point</span>
<span class="k">var</span> v <span class="o">=</span> AD.Hessian(x <span class="o">=</span><span class="o">&gt;</span> AD.Sin(x[0] <span class="o">*</span> x[1]), <span class="k">new</span> D[] { 3, 2 });
</pre></td></tr></table>

<h3>AD.Hessianv</h3>

<h4>Hessian-vector product</h4>

<p>Syntax: <code>public static D[] AD.Hessianv(Func&lt;D[],D&gt; f, D[] x, D[] v)</code></p>

<p>For a function <span class="math">\(f: \mathbb{R}^n \to \mathbb{R}\)</span>, and <span class="math">\(\mathbf{x}, \mathbf{v} \in \mathbb{R}^n\)</span>, this returns the <a href="https://en.wikipedia.org/wiki/Hessian_automatic_differentiation">Hessian-vector product</a>, that is, the multiplication of the Hessian matrix of <span class="math">\(f\)</span> at <span class="math">\(\mathbf{x}\)</span> with <span class="math">\(\mathbf{v}\)</span></p>

<p><span class="math">\[  \left( \mathbf{H}_f \right)_\mathbf{x} \; \mathbf{v} \; .\]</span></p>

<p>With AD, this value is computed efficiently using one forward and one reverse evaluation of the function, in a matrix-free way (without computing the full Hessian matrix).</p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
<span class="l">2: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Hessian-vector product of a vector-to-scalar function</span>
<span class="k">var</span> hv <span class="o">=</span> AD.Hessianv(x <span class="o">=</span><span class="o">&gt;</span> AD.Sin(x[0] <span class="o">*</span> x[1]), <span class="k">new</span> D[] { 3, 2 }, <span class="k">new</span> D[] { 5, 3 });
</pre></td></tr></table>

<h3>AD.Laplacian</h3>

<h4>Laplacian of a vector-to-scalar function</h4>

<p>Syntax: <code>public static Func&lt;D[],D&gt; AD.Laplacian(Func&lt;D[],D&gt; f)</code></p>

<p>For a function <span class="math">\(f(a_1, \dots, a_n): \mathbb{R}^n \to \mathbb{R}\)</span>, and <span class="math">\(\mathbf{x} \in \mathbb{R}^n\)</span>, this returns a function that computes the sum of second derivatives evaluated at <span class="math">\(\mathbf{x}\)</span></p>

<p><span class="math">\[  \mathrm{tr}\left(\mathbf{H}_f \right) = \left(\frac{\partial ^2 f}{\partial a_1^2} + \dots + \frac{\partial ^2 f}{\partial a_n^2}\right) \; ,\]</span></p>

<p>which is the trace of the Hessian matrix.</p>

<p>With AD, this value is computed efficiently in a Matrix-free way, without computing the full Hessian matrix.</p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
<span class="l">2: </span>
<span class="l">3: </span>
<span class="l">4: </span>
<span class="l">5: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Laplacian of a vector-to-scalar function</span>
<span class="k">var</span> lf <span class="o">=</span> AD.Laplacian(x <span class="o">=</span><span class="o">&gt;</span> AD.Sin(x[0] <span class="o">*</span> x[1]));

<span class="c">// Evaluate lf at a point</span>
<span class="k">var</span> v <span class="o">=</span> lf(<span class="k">new</span> D[] { 3, 2 });
</pre></td></tr></table>

<h4>Laplacian of a vector-to-scalar function evaluated at a point</h4>

<p>Syntax: <code>public static D AD.Laplacian(Func&lt;D[],D&gt; f, D[] x)</code></p>

<p>For a function <span class="math">\(f(a_1, \dots, a_n): \mathbb{R}^n \to \mathbb{R}\)</span>, and <span class="math">\(\mathbf{x} \in \mathbb{R}^n\)</span>, this returns the sum of second derivatives evaluated at <span class="math">\(\mathbf{x}\)</span></p>

<p><span class="math">\[  \mathrm{tr}\left(\mathbf{H}_f \right)_\mathbf{x} = \left. \left(\frac{\partial ^2 f}{\partial a_1^2} + \dots + \frac{\partial ^2 f}{\partial a_n^2}\right) \right|_{\mathbf{a} \; = \; \mathbf{x}} \; .\]</span></p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
<span class="l">2: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Laplacian of a vector-to-scalar function at a point</span>
<span class="k">var</span> v <span class="o">=</span> AD.Laplacian(x <span class="o">=</span><span class="o">&gt;</span> AD.Sin(x[0] <span class="o">*</span> x[1]), <span class="k">new</span> D[] { 3, 2 });
</pre></td></tr></table>

<h3>AD.Jacobian</h3>

<h4>Jacobian of a vector-to-vector function</h4>

<p>Syntax: <code>public static Func&lt;D[],D[,]&gt; AD.Jacobian(Func&lt;D[],D[]&gt; f)</code></p>

<p>For a function <span class="math">\(\mathbf{F}: \mathbb{R}^n \to \mathbb{R}^m\)</span> with components <span class="math">\(F_1 (a_1, \dots, a_n), \dots, F_m (a_1, \dots, a_n)\)</span>, this returns a function that computes the <span class="math">\(m\)</span>-by-<span class="math">\(n\)</span> <a href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant">Jacobian matrix</a></p>

<p><span class="math">\[  \mathbf{J}_\mathbf{F} = \begin{bmatrix}
                            \frac{\partial F_1}{\partial a_1} &amp; \cdots &amp; \frac{\partial F_1}{\partial a_n} \\
                            \vdots &amp; \ddots &amp; \vdots  \\
                            \frac{\partial F_m}{\partial a_1} &amp; \cdots &amp; \frac{\partial F_m}{\partial a_n}
                            \end{bmatrix} \; .\]</span></p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
<span class="l">2: </span>
<span class="l">3: </span>
<span class="l">4: </span>
<span class="l">5: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Jacobian of a vector-to-vector function</span>
<span class="k">var</span> jf <span class="o">=</span> AD.Jacobian(x <span class="o">=</span><span class="o">&gt;</span> <span class="k">new</span> D[] { AD.Sin(x[0] <span class="o">*</span> x[1]), x[0] <span class="o">-</span> x[1], x[2] });

<span class="c">// Evaluate jf at a point</span>
<span class="k">var</span> v <span class="o">=</span> jf(<span class="k">new</span> D[] { 3, 2, 4 });
</pre></td></tr></table>

<h4>Jacobian of a vector-to-vector function evaluated at a point</h4>

<p>Syntax: <code>public static D[,] AD.Jacobian(Func&lt;D[],D[]&gt; f, D[] x)</code></p>

<p>For a function <span class="math">\(\mathbf{F}: \mathbb{R}^n \to \mathbb{R}^m\)</span> with components <span class="math">\(F_1 (a_1, \dots, a_n), \dots, F_m (a_1, \dots, a_n)\)</span>, and <span class="math">\(\mathbf{x} \in \mathbb{R}^n\)</span>, this returns the <span class="math">\(m\)</span>-by-<span class="math">\(n\)</span> Jacobian matrix evaluated at <span class="math">\(\mathbf{x}\)</span></p>

<p><span class="math">\[  \left( \mathbf{J}_\mathbf{F} \right)_\mathbf{x} = \left. \begin{bmatrix}
                                                            \frac{\partial F_1}{\partial a_1} &amp; \cdots &amp; \frac{\partial F_1}{\partial a_n} \\
                                                            \vdots &amp; \ddots &amp; \vdots  \\
                                                            \frac{\partial F_m}{\partial a_1} &amp; \cdots &amp; \frac{\partial F_m}{\partial a_n}
                                                           \end{bmatrix} \right|_{\mathbf{a}\; = \; \mathbf{x}} \; .\]</span></p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
<span class="l">2: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Jacobian of a vector-to-vector function at a point</span>
<span class="k">var</span> v <span class="o">=</span> AD.Jacobian(x <span class="o">=</span><span class="o">&gt;</span> <span class="k">new</span> D[] { AD.Sin(x[0] <span class="o">*</span> x[1]), x[0] <span class="o">-</span> x[1], x[2] }, <span class="k">new</span> D[] { 3, 2, 4 });
</pre></td></tr></table>

<h3>AD.Jacobianv</h3>

<h4>Jacobian-vector product</h4>

<p>Syntax: <code>public static D[] AD.Jacobianv(Func&lt;D[],D[]&gt; f, D[] x, D[] v)</code></p>

<p>For a function <span class="math">\(\mathbf{F}: \mathbb{R}^n \to \mathbb{R}^m\)</span>, and <span class="math">\(\mathbf{x}, \mathbf{v} \in \mathbb{R}^n\)</span>, this returns the Jacobian-vector product, that is, the matrix product of the Jacobian of <span class="math">\(\mathbf{F}\)</span> at <span class="math">\(\mathbf{x}\)</span> with <span class="math">\(\mathbf{v}\)</span></p>

<p><span class="math">\[  \left( \mathbf{J}_\mathbf{F} \right)_\mathbf{x} \mathbf{v} \; .\]</span></p>

<p>With AD, this value is computed efficiently in one forward evaluation of the function, in a matrix-free way (without computing the full Jacobian matrix).</p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
<span class="l">2: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Jacobian-vector product of a vector-to-vector function</span>
<span class="k">var</span> v <span class="o">=</span> AD.Jacobianv(x <span class="o">=</span><span class="o">&gt;</span> <span class="k">new</span> D[] { AD.Sin(x[0] <span class="o">*</span> x[1]), x[0] <span class="o">-</span> x[1], x[2] }, <span class="k">new</span> D[] { 3, 2, 4 }, <span class="k">new</span> D[] { 1, 2, 3 });
</pre></td></tr></table>

<h3>AD.JacobianT</h3>

<h4>Transposed Jacobian of a vector-to-vector function</h4>

<p>Syntax: <code>public static Func&lt;D[],D[,]&gt; AD.JacobianT(Func&lt;D[],D[]&gt; f)</code></p>

<p>For a function <span class="math">\(\mathbf{F}: \mathbb{R}^n \to \mathbb{R}^m\)</span> with components <span class="math">\(F_1 (a_1, \dots, a_n), \dots, F_m (a_1, \dots, a_n)\)</span>, this returns a function that computes the <span class="math">\(n\)</span>-by-<span class="math">\(m\)</span> transposed Jacobian matrix</p>

<p><span class="math">\[  \mathbf{J}_\mathbf{F}^\textrm{T} = \begin{bmatrix}
                                        \frac{\partial F_1}{\partial a_1} &amp; \cdots &amp; \frac{\partial F_m}{\partial a_1} \\
                                        \vdots &amp; \ddots &amp; \vdots  \\
                                        \frac{\partial F_1}{\partial a_n} &amp; \cdots &amp; \frac{\partial F_m}{\partial a_n}
                                        \end{bmatrix} \; .\]</span></p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
<span class="l">2: </span>
<span class="l">3: </span>
<span class="l">4: </span>
<span class="l">5: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Transposed Jacobian of a vector-to-vector function</span>
<span class="k">var</span> jf <span class="o">=</span> AD.JacobianT(x <span class="o">=</span><span class="o">&gt;</span> <span class="k">new</span> D[] { AD.Sin(x[0] <span class="o">*</span> x[1]), x[0] <span class="o">-</span> x[1], x[2] });

<span class="c">// Evaluate jf at a point</span>
<span class="k">var</span> v <span class="o">=</span> jf(<span class="k">new</span> D[] { 3, 2, 4 });
</pre></td></tr></table>

<h4>Transposed Jacobian of a vector-to-vector function evaluated at a point</h4>

<p>Syntax: <code>public static D[,] AD.JacobianT(Func&lt;D[],D[]&gt; f, D[] x)</code></p>

<p>For a function <span class="math">\(\mathbf{F}: \mathbb{R}^n \to \mathbb{R}^m\)</span> with components <span class="math">\(F_1 (a_1, \dots, a_n), \dots, F_m (a_1, \dots, a_n)\)</span>, and <span class="math">\(\mathbf{x} \in \mathbb{R}^n\)</span>, this returns the <span class="math">\(n\)</span>-by-<span class="math">\(m\)</span> transposed Jacobian matrix evaluated at <span class="math">\(\mathbf{x}\)</span></p>

<p><span class="math">\[  \left( \mathbf{J}_\mathbf{F}^\textrm{T} \right)_\mathbf{x} = \left. \begin{bmatrix}
                                                            \frac{\partial F_1}{\partial a_1} &amp; \cdots &amp; \frac{\partial F_m}{\partial a_1} \\
                                                            \vdots &amp; \ddots &amp; \vdots  \\
                                                            \frac{\partial F_1}{\partial a_n} &amp; \cdots &amp; \frac{\partial F_m}{\partial a_n}
                                                           \end{bmatrix} \right|_{\mathbf{a}\; = \; \mathbf{x}} \; .\]</span></p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
<span class="l">2: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Transposed Jacobian of a vector-to-vector function at a point</span>
<span class="k">var</span> v <span class="o">=</span> AD.JacobianT(x <span class="o">=</span><span class="o">&gt;</span> <span class="k">new</span> D[] { AD.Sin(x[0] <span class="o">*</span> x[1]), x[0] <span class="o">-</span> x[1], x[2] }, <span class="k">new</span> D[] { 3, 2, 4 });
</pre></td></tr></table>

<h3>AD.JacobianTv</h3>

<h4>Transposed Jacobian-vector product</h4>

<p>Syntax: <code>public static D[] AD.JacobianTv(Func&lt;D[],D[]&gt; f, D[] x, D[] v)</code></p>

<p>For a function <span class="math">\(\mathbf{F}: \mathbb{R}^n \to \mathbb{R}^m\)</span>, <span class="math">\(\mathbf{x} \in \mathbb{R}^n\)</span>, and <span class="math">\(\mathbf{v} \in \mathbb{R}^m\)</span>, this returns the matrix product of the transposed Jacobian of <span class="math">\(\mathbf{F}\)</span> at <span class="math">\(\mathbf{x}\)</span> with <span class="math">\(\mathbf{v}\)</span></p>

<p><span class="math">\[  \left( \mathbf{J}_\mathbf{F}^\textrm{T} \right)_\mathbf{x} \mathbf{v} \; .\]</span></p>

<p>With AD, this value is computed efficiently in one forward and one reverse evaluation of the function, in a matrix-free way (without computing the full Jacobian matrix).</p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
<span class="l">2: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Transposed Jacobian-vector product of a vector-to-vector function</span>
<span class="k">var</span> v <span class="o">=</span> AD.JacobianTv(x <span class="o">=</span><span class="o">&gt;</span> <span class="k">new</span> D[] { AD.Sin(x[0] <span class="o">*</span> x[1]), x[0] <span class="o">-</span> x[1], x[2] }, <span class="k">new</span> D[] { 3, 2, 4 }, <span class="k">new</span> D[] { 1, 2, 3 });
</pre></td></tr></table>

<h3>AD.Curl</h3>

<h4>Curl of a vector-to-vector function</h4>

<p>Syntax: <code>public static Func&lt;D[],D[]&gt; AD.Curl(Func&lt;D[],D[]&gt; f)</code></p>

<p>For a function <span class="math">\(\mathbf{F}: \mathbb{R}^3 \to \mathbb{R}^3\)</span> with components <span class="math">\(F_1(a_1, a_2, a_3),\; F_2(a_1, a_2, a_3),\; F_3(a_1, a_2, a_3)\)</span> this returns a function that computes the <a href="https://en.wikipedia.org/wiki/Curl_(mathematics)">curl</a>, that is,</p>

<p><span class="math">\[  \textrm{curl} \, \mathbf{F} = \nabla \times \mathbf{F} = \left[ \frac{\partial F_3}{\partial a_2} - \frac{\partial F_2}{\partial a_3}, \; \frac{\partial F_1}{\partial a_3} - \frac{\partial F_3}{\partial a_1}, \; \frac{\partial F_2}{\partial a_1} - \frac{\partial F_1}{\partial a_2} \right] \; .\]</span></p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
<span class="l">2: </span>
<span class="l">3: </span>
<span class="l">4: </span>
<span class="l">5: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Curl of a vector-to-vector function</span>
<span class="k">var</span> cf <span class="o">=</span> AD.Curl(x <span class="o">=</span><span class="o">&gt;</span> <span class="k">new</span> D[] { AD.Sin(x[0] <span class="o">*</span> x[1]), x[0] <span class="o">-</span> x[1], x[2] });

<span class="c">// Evaluate cf at a point</span>
<span class="k">var</span> v <span class="o">=</span> cf(<span class="k">new</span> D[] { 3, 2, 4 });
</pre></td></tr></table>

<h4>Curl of a vector-to-vector function evaluated at a point</h4>

<p>Syntax: <code>public static D[] AD.Curl(Func&lt;D[],D[]&gt; f, D[] x)</code></p>

<p>For a function <span class="math">\(\mathbf{F}: \mathbb{R}^3 \to \mathbb{R}^3\)</span> with components <span class="math">\(F_1(a_1, a_2, a_3),\; F_2(a_1, a_2, a_3),\; F_3(a_1, a_2, a_3)\)</span>, and <span class="math">\(\mathbf{x} \in \mathbb{R}^3\)</span>, this returns the curl evaluated at <span class="math">\(\mathbf{x}\)</span></p>

<p><span class="math">\[  \left( \textrm{curl} \, \mathbf{F} \right)_{\mathbf{x}} = \left( \nabla \times \mathbf{F} \right)_{\mathbf{x}}= \left. \left[ \frac{\partial F_3}{\partial a_2} - \frac{\partial F_2}{\partial a_3}, \; \frac{\partial F_1}{\partial a_3} - \frac{\partial F_3}{\partial a_1}, \; \frac{\partial F_2}{\partial a_1} - \frac{\partial F_1}{\partial a_2} \right] \right|_{\mathbf{a}\; = \; \mathbf{x}} \; .\]</span></p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
<span class="l">2: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Curl of a vector-to-vector function at a point</span>
<span class="k">var</span> v <span class="o">=</span> AD.Curl(x <span class="o">=</span><span class="o">&gt;</span> <span class="k">new</span> D[] { AD.Sin(x[0] <span class="o">*</span> x[1]), x[0] <span class="o">-</span> x[1], x[2] }, <span class="k">new</span> D[] { 3, 2, 4 });
</pre></td></tr></table>

<h3>AD.Div</h3>

<h4>Divergence of a vector-to-vector function</h4>

<p>Syntax: <code>public static Func&lt;D[],D&gt; AD.Div(Func&lt;D[],D[]&gt; f)</code></p>

<p>For a function <span class="math">\(\mathbf{F}: \mathbb{R}^n \to \mathbb{R}^n\)</span> with components <span class="math">\(F_1(a_1, \dots, a_n),\; \dots, \; F_n(a_1, \dots, a_n)\)</span>, this returns a function that computes the <a href="https://en.wikipedia.org/wiki/Divergence">divergence</a>, that is, the trace of the Jacobian matrix</p>

<p><span class="math">\[  \textrm{div} \, \mathbf{F} = \nabla \cdot \mathbf{F} = \textrm{tr}\left( \mathbf{J}_{\mathbf{F}} \right) = \left( \frac{\partial F_1}{\partial a_1} + \dots + \frac{\partial F_n}{\partial a_n}\right) \; .\]</span></p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
<span class="l">2: </span>
<span class="l">3: </span>
<span class="l">4: </span>
<span class="l">5: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Divergence of a vector-to-vector function</span>
<span class="k">var</span> df <span class="o">=</span> AD.Curl(x <span class="o">=</span><span class="o">&gt;</span> <span class="k">new</span> D[] { AD.Sin(x[0] <span class="o">*</span> x[1]), x[0] <span class="o">-</span> x[1], x[2] });

<span class="c">// Evaluate df at a point</span>
<span class="k">var</span> v <span class="o">=</span> df(<span class="k">new</span> D[] { 3, 2, 4 });
</pre></td></tr></table>

<h4>Divergence of a vector-to-vector function evaluated at a point</h4>

<p>Syntax: <code>public static D AD.Div(Func&lt;D[],D[]&gt; f, D[] x)</code></p>

<p>For a function <span class="math">\(\mathbf{F}: \mathbb{R}^n \to \mathbb{R}^n\)</span> with components <span class="math">\(F_1(a_1, \dots, a_n),\; \dots, \; F_n(a_1, \dots, a_n)\)</span>, and <span class="math">\(\mathbf{x} \in \mathbb{R}^n\)</span>, this returns the trace of the Jacobian matrix evaluated at <span class="math">\(\mathbf{x}\)</span></p>

<p><span class="math">\[  \left( \textrm{div} \, \mathbf{F} \right)_{\mathbf{x}} = \left( \nabla \cdot \mathbf{F} \right)_{\mathbf{x}} = \textrm{tr}\left( \mathbf{J}_{\mathbf{F}} \right)_{\mathbf{x}} = \left. \left( \frac{\partial F_1}{\partial a_1} + \dots + \frac{\partial F_n}{\partial a_n}\right) \right|_{\mathbf{a}\; = \; \mathbf{x}} \; .\]</span></p>

<table class="pre"><tr><td class="lines"><span class="l">1: </span>
<span class="l">2: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="c">// Divergence of a vector-to-vector function at a point</span>
<span class="k">var</span> v <span class="o">=</span> AD.Curl(x <span class="o">=</span><span class="o">&gt;</span> <span class="k">new</span> D[] { AD.Sin(x[0] <span class="o">*</span> x[1]), x[0] <span class="o">-</span> x[1], x[2] }, <span class="k">new</span> D[] { 3, 2, 4 });
</pre></td></tr></table>

<h2>Numerical Differentiation</h2>

<p><strong>DiffSharp.Interop</strong> also provides access to <a href="gettingstarted-numericaldifferentiation.html">numerical differentiation</a>, through the <strong>DiffSharp.Interop.Numerical</strong> class.</p>

<p>Numerical differentiation operations are used with the <strong>double</strong> numeric type, and the common mathematical functions can be accessed using the <a href="https://msdn.microsoft.com/en-us/library/System.Math(v=vs.110).aspx"><strong>System.Math</strong></a> class as usual (e.g. <strong>Math.Exp</strong>, <strong>Math.Sin</strong>, <strong>Math.Pow</strong> ).</p>

<p>Currently, the following operations are supported:</p>

<ul>
<li><strong>AD.Diff</strong>: First derivative of a scalar-to-scalar function</li>
<li><strong>AD.Diff2</strong>: Second derivative of a scalar-to-scalar function</li>
<li><strong>AD.Grad</strong>: Gradient of a vector-to-scalar function</li>
<li><strong>AD.Gradv</strong>: Gradient-vector product (directional derivative)</li>
<li><strong>AD.Hessian</strong>: Hessian of a vector-to-scalar function</li>
<li><strong>AD.Hessianv</strong>: Hessian-vector product</li>
<li><strong>AD.Laplacian</strong>: Laplacian of a vector-to-scalar function</li>
<li><strong>AD.Jacobian</strong>: Jacobian of a vector-to-vector function</li>
<li><strong>AD.JacobianT</strong>: Transposed Jacobian of a vector-to-vector function</li>
<li><strong>AD.Jacobianv</strong>: Jacobian-vector product</li>
<li><strong>AD.Curl</strong>: Curl of a vector-to-vector function</li>
<li><strong>AD.Div</strong>: Divergence of a vector-to-vector function</li>
</ul>

<p>Here are some examples:</p>

<table class="pre"><tr><td class="lines"><span class="l"> 1: </span>
<span class="l"> 2: </span>
<span class="l"> 3: </span>
<span class="l"> 4: </span>
<span class="l"> 5: </span>
<span class="l"> 6: </span>
<span class="l"> 7: </span>
<span class="l"> 8: </span>
<span class="l"> 9: </span>
<span class="l">10: </span>
<span class="l">11: </span>
<span class="l">12: </span>
<span class="l">13: </span>
<span class="l">14: </span>
<span class="l">15: </span>
<span class="l">16: </span>
<span class="l">17: </span>
<span class="l">18: </span>
<span class="l">19: </span>
<span class="l">20: </span>
<span class="l">21: </span>
<span class="l">22: </span>
<span class="l">23: </span>
<span class="l">24: </span>
<span class="l">25: </span>
<span class="l">26: </span>
<span class="l">27: </span>
<span class="l">28: </span>
<span class="l">29: </span>
<span class="l">30: </span>
<span class="l">31: </span>
<span class="l">32: </span>
<span class="l">33: </span>
<span class="l">34: </span>
<span class="l">35: </span>
<span class="l">36: </span>
<span class="l">37: </span>
<span class="l">38: </span>
<span class="l">39: </span>
<span class="l">40: </span>
<span class="l">41: </span>
<span class="l">42: </span>
<span class="l">43: </span>
<span class="l">44: </span>
<span class="l">45: </span>
<span class="l">46: </span>
<span class="l">47: </span>
<span class="l">48: </span>
<span class="l">49: </span>
<span class="l">50: </span>
<span class="l">51: </span>
<span class="l">52: </span>
<span class="l">53: </span>
<span class="l">54: </span>
<span class="l">55: </span>
<span class="l">56: </span>
<span class="l">57: </span>
<span class="l">58: </span>
<span class="l">59: </span>
<span class="l">60: </span>
<span class="l">61: </span>
<span class="l">62: </span>
<span class="l">63: </span>
<span class="l">64: </span>
</td>
<td class="snippet"><pre lang="csharp"><span class="k">using</span> DiffSharp.Interop;

<span class="k">class</span> Program
{
    <span class="c">// A scalar-to-scalar function</span>
    <span class="c">// F(x) = Sin(x^2 - Exp(x))</span>
    <span class="k">public</span> <span class="k">static</span> <span class="k">double</span> F(<span class="k">double</span> x)
    {
        <span class="k">return</span> Math.Sin(x <span class="o">*</span> x <span class="o">-</span> Math.Exp(x));
    }

    <span class="c">// A vector-to-scalar function</span>
    <span class="c">// G(x1, x2) = Sin(x1 * x2)</span>
    <span class="k">public</span> <span class="k">static</span> <span class="k">double</span> G(<span class="k">double</span>[] x)
    {
        <span class="k">return</span> Math.Sin(x[0] <span class="o">*</span> x[1]);
    }

    <span class="c">// A vector-to-vector function</span>
    <span class="c">// H(x1, x2, x3) = (Sin(x1 * x2), Exp(x1 - x2), x3)</span>
    <span class="k">public</span> <span class="k">static</span> <span class="k">double</span>[] H(<span class="k">double</span>[] x)
    {
        <span class="k">return</span> <span class="k">new</span> <span class="k">double</span>[] { Math.Sin(x[0] <span class="o">*</span> x[1]), Math.Exp(x[0] <span class="o">-</span> x[1]), x[2] };
    }

    <span class="k">public</span> <span class="k">static</span> <span class="k">void</span> Main(<span class="k">string</span>[] args)
    {
        <span class="c">// Derivative of F(x) at x = 3</span>
        <span class="k">var</span> a <span class="o">=</span> Numerical.Diff(F, 3);

        <span class="c">// Second derivative of F(x) at x = 3</span>
        <span class="k">var</span> b <span class="o">=</span> Numerical.Diff2(F, 3);

        <span class="c">// Gradient of G(x) at x = (4, 3)</span>
        <span class="k">var</span> c <span class="o">=</span> Numerical.Grad(G, <span class="k">new</span> <span class="k">double</span>[] { 4, 3 });

        <span class="c">// Directional derivative of G(x) at x = (4, 3) along v = (2, 5)</span>
        <span class="k">var</span> d <span class="o">=</span> Numerical.Gradv(G, <span class="k">new</span> <span class="k">double</span>[] { 4, 3 }, <span class="k">new</span> <span class="k">double</span>[] { 2, 5 });

        <span class="c">// Hessian of G(x) at x = (4, 3)</span>
        <span class="k">var</span> e <span class="o">=</span> Numerical.Hessian(G, <span class="k">new</span> <span class="k">double</span>[] { 4, 3 });

        <span class="c">// Hessian-vector product of G(x), with x = (4, 3) and v = (2, 5)</span>
        <span class="k">var</span> f <span class="o">=</span> Numerical.Hessianv(G, <span class="k">new</span> <span class="k">double</span>[] { 4, 3 }, <span class="k">new</span> <span class="k">double</span>[] { 2, 5 });

        <span class="c">// Laplacian of G(x) at x = (4, 3)</span>
        <span class="k">var</span> g <span class="o">=</span> Numerical.Laplacian(G, <span class="k">new</span> <span class="k">double</span>[] { 4, 3 });

        <span class="c">// Jacobian of H(x) at x = (5, 2, 1)</span>
        <span class="k">var</span> h <span class="o">=</span> Numerical.Jacobian(H, <span class="k">new</span> <span class="k">double</span>[] { 5, 2, 1 });

        <span class="c">// Transposed Jacobian of H(x) at x = (5, 2, 1)</span>
        <span class="k">var</span> i <span class="o">=</span> Numerical.JacobianT(H, <span class="k">new</span> <span class="k">double</span>[] { 5, 2, 1 });

        <span class="c">// Jacobian-vector product of H(x), with x = (5, 2, 1) and v = (2, 5, 3)</span>
        <span class="k">var</span> j <span class="o">=</span> Numerical.Jacobianv(H, <span class="k">new</span> <span class="k">double</span>[] { 5, 2, 1 }, <span class="k">new</span> <span class="k">double</span>[] { 2, 5, 3 });

        <span class="c">// Curl of H(x) at x = (5, 2, 1)</span>
        <span class="k">var</span> k <span class="o">=</span> Numerical.Curl(H, <span class="k">new</span> <span class="k">double</span>[] { 5, 2, 1 });

        <span class="c">// Divergence of H(x) at x = (5, 2, 1)</span>
        <span class="k">var</span> l <span class="o">=</span> Numerical.Div(H, <span class="k">new</span> <span class="k">double</span>[] { 5, 2, 1 });
    }
}</pre></td></tr></table>

                    
        </div>
        <div class="span3">
          <a href="index.html"><img src="img/diffsharp-logo.png" style="width:140px;height:140px;margin:10px 0px 0px 20px;border-style:none;"/></a>

          <ul class="nav nav-list" id="menu">
            <li class="nav-header">DiffSharp</li>
            <li class="divider"></li>
            <li><a href="index.html">Home Page</a></li>
            <li><a href="https://www.nuget.org/packages/diffsharp">Get DiffSharp via NuGet</a></li>
            <li><a href="https://github.com/DiffSharp/DiffSharp">GitHub Page</a></li>
            <li><a href="https://github.com/DiffSharp/DiffSharp/releases">Release Notes</a></li>

            <li class="nav-header">Getting Started</li>
            <li class="divider"></li>
            <li><a href="gettingstarted-typeinference.html">Type Inference</a></li>
            <li><a href="api-overview.html">API Overview</a></li>
            <li><a href="gettingstarted-nestedad.html">Nested AD</a></li>
            <li><a href="gettingstarted-nonnestedad.html">Non-nested AD</a></li>
            <li><a href="gettingstarted-numericaldifferentiation.html">Numerical Differentiation</a></li>
            <li><a href="gettingstarted-symbolicdifferentiation.html">Symbolic Differentiation</a></li>
            <li><a href="benchmarks.html">Benchmarks</a></li>
            <li><a href="csharp.html">C# and Other Languages</a></li>            
            <li><a href="reference/index.html">API Reference</a></li>

            <li class="nav-header">Examples</li>
            <li class="divider"></li>

            <li class="nav-header">Machine Learning</li>
            <li><a href="examples-gradientdescent.html">Gradient Descent</a></li>
            <li><a href="examples-newtonsmethod.html">Newton's Method</a></li>
            <li><a href="examples-stochasticgradientdescent.html">Stochastic Gradient Descent</a></li>
            <li><a href="examples-kmeansclustering.html">K-Means Clustering</a></li>
            <li><a href="examples-hamiltonianmontecarlo.html">Hamiltonian Monte Carlo</a></li>
            <li><a href="examples-neuralnetworks.html">Neural Networks</a></li>
            <li>Neural Turing Machines (to come)</li>
            <li>Probabilistic Programming<br>(to come)</li>

            <li class="nav-header">Dynamical Systems</li>
            <li>Stability Analysis (to come)</li>

            <li class="nav-header">Control</li>
            <li><a href="examples-inversekinematics.html">Inverse Kinematics</a></li>
            <li>Adaptive Control (to come)</li>

            <li class="nav-header">Physics</li>
            <li><a href="examples-kinematics.html">Kinematics</a></li>
            <li>Leapfrog Integration (to come)</li>
            <li><a href="examples-helmholtzenergyfunction.html">Helmholtz Energy Function</a></li>

            <li class="nav-header">Math</li>
            <li><a href="examples-lhopitalsrule.html">l'Hôpital's Rule</a></li>

            <li class="nav-header">Makers</li>
            <li class="divider"></li>
            <li><a href="https://www.cs.nuim.ie/~gunes/">Atılım Güneş Baydin</a></li>
            <li><a href="http://www.bcl.hamilton.ie/~barak/">Barak A. Pearlmutter</a></li>
            <li><a href="http://www.bcl.hamilton.ie/">Brain and Computation Lab</a></li>
          </ul>
        </div>
      </div>
    </div>
    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=10059115; 
    var sc_invisible=1; 
    var sc_security="92275ee1"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="https://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="https://c.statcounter.com/10059115/0/92275ee1/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->
  </body>
</html>