
        {
            "cells": [
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["Gradient Descent\n",
"================\n",
"\n",
"The [gradient descent algorithm](https://en.wikipedia.org/wiki/Gradient_descent) is an optimization algorithm for finding a local minimum of a scalar-valued function near a starting point, taking successive steps in the direction of the negative of the gradient.\n",
"\n",
"For a function $f: \\mathbb{R}^n \\to \\mathbb{R}$, starting from an initial point $\\mathbf{x}_0$, the method works by computing successive points in the function domain\n",
"\n",
"\begin{equation}\n",
" \\mathbf{x}_{n + 1} = \\mathbf{x}_n - \\eta \\left( \\nabla f \\right)_{\\mathbf{x}_n} \\; ,\n",
"\\end{equation}\n",
"\n",
"where $\\eta \u003e 0$ is a small step size and $\\left( \\nabla f \\right)_{\\mathbf{x}_n}$ is the [gradient](https://en.wikipedia.org/wiki/Gradient) of $f$ evaluated at $\\mathbf{x}_n$. The successive values of the function \n",
"\n",
"\begin{equation}\n",
" f(\\mathbf{x}_0) \\ge f(\\mathbf{x}_1) \\ge f(\\mathbf{x}_2) \\ge \\dots\n",
"\\end{equation}\n",
" \n",
"keep decreasing and the sequence $\\mathbf{x}_n$ usually converges to a local minimum.\n",
"\n",
"In practice, using a fixed step size $\\eta$ yields suboptimal performance and there are adaptive algorithms that select a locally optimal step size $\\eta$ on each iteration.\n",
"\n",
"The following code implements gradient descent with fixed step size, stopping when the [norm](https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm) of the gradient falls below a given threshold.\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 2, "outputs": [], 
           "source": ["open DiffSharp\n",
"\n",
"// Shorthand for dsharp.tensor\n",
"let t x = dsharp.tensor x\n",
"\n",
"// Gradient descent\n",
"//   f: function\n",
"//   x0: starting point\n",
"//   eta: step size\n",
"//   epsilon: threshold\n",
"let rec gradientDescent f x eta epsilon =\n",
"    let g = dsharp.grad f x\n",
"    if g.norm \u003c epsilon then x \n",
"    else gradientDescent f (x - eta * g) eta epsilon\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["Let\u0027s find a minimum of $f(x, y) = (\\sin x + \\cos y)$.\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 3, "outputs": [
          {
           "data": {
            "text/plain": ["val xmin : Tensor = tensor [ -1.570790759; 3.141591964 ]",
"",
"val fxmin : Tensor = tensor -2.0"]
        },
           "execution_count": 3,
           "metadata": {},
           "output_type": "execute_result"
          }], 
           "source": ["let inline f (x:Tensor) =  sin x.[0] + cos x.[1]\n",
"\n",
"// Find the minimum of f\n",
"// Start from (1, 1), step size 0.9, threshold 0.00001\n",
"let xmin = gd f (t [1.; 1.]) (t 0.9) (t 0.00001)\n",
"let fxmin = f xmin\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["A minimum, $f(x, y) = -2$, is found at $(x, y) = \\left(-\\frac{\\pi}{2}, \\pi\\right)$.\n",
"\n"]
          }],
            "metadata": {
            "kernelspec": {"display_name": ".NET (F#)", "language": "F#", "name": ".net-fsharp"},
            "langauge_info": {
        "file_extension": ".fs",
        "mimetype": "text/x-fsharp",
        "name": "C#",
        "pygments_lexer": "fsharp",
        "version": "4.5"
        }
        },
            "nbformat": 4,
            "nbformat_minor": 1
        }
        
