
        {
            "cells": [
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["Nested AD\n",
"=========\n",
"\n",
"The main functionality of DiffSharp is found under the **DiffSharp.AD** namespace. Opening this namespace allows you to automatically evaluate derivatives of functions via forward and/or reverse AD. Internally, for any case involving a function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$, DiffSharp uses forward AD when $n \\ll m$ and reverse AD when $n \\gg m$. Combinations such as reverse-on-forward or forward-on-reverse AD can be also handled.\n",
"\n",
"For a complete list of the available differentiation operations, please refer to [API Overview](api-overview.html) and [API Reference](reference/index.html).\n",
"\n",
"Background\n",
"----------\n",
"\n",
"The library supports nested invocations of differentiation operations. So, for example, you can compute exact higher-order derivatives or take derivatives of functions that are themselves internally computing derivatives. \n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 2, "outputs": [], 
           "source": ["open DiffSharp.AD.Float64\n",
"\n",
"let y x = sin (sqrt x)\n",
"\n",
"// Derivative of y\n",
"let d1 = diff y\n",
"\n",
"// 2nd derivative of y\n",
"let d2 = diff (diff y)\n",
"\n",
"// 3rd derivative of y\n",
"let d3 = diff d2\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["Nesting capability means more than just being able to compute higher-order derivatives of one function.\n",
"\n",
"DiffSharp can handle complex nested cases such as computing the derivative of a function $f$ that takes an argument $x$, which, in turn, computes the derivative of another function $g$ nested inside $f$ that has a free reference to $x$, the argument to the surrounding function.\n",
"\n",
"\begin{equation}\n",
"  \\frac{d}{dx} \\left. \\left( x \\left( \\left. \\frac{d}{dy} x y \\; \\right|_{y=3} \\right) \\right) \\right|_{x=2}\n",
"\\end{equation}\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 3, "outputs": [
          {
           "data": {
            "text/plain": ["val d4 : D = D 4.0"]
        },
           "execution_count": 3,
           "metadata": {},
           "output_type": "execute_result"
          }], 
           "source": ["let d4 = diff (fun x -\u003e x * (diff (fun y -\u003e x * y) (D 3.))) (D 2.)\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["This allows you to write, for example, nested optimization algorithms of the form\n",
"\n",
"\begin{equation}\n",
"  \\mathbf{min} \\left( \\lambda x \\; . \\; (f \\; x) + \\mathbf{min} \\left( \\lambda y \\; . \\; g \\; x \\; y \\right) \\right)\\; ,\n",
"\\end{equation}\n",
"\n",
"for functions $f$ and $g$ and a gradient-based minimization procedure $\\mathbf{min}$.\n",
"\n",
"Correctly nesting AD in a functional framework is achieved through the method of \"tagging\", which serves to prevent a class of bugs called \"perturbation confusion\" where a system fails to distinguish between distinct perturbations introduced by distinct invocations of differentiation operations. You can refer to the following articles, among others, to understand the issue and how it should be handled correctly:\n",
"\n",
"\n",
"_Jeffrey Mark Siskind and Barak A. Pearlmutter. Perturbation Confusion and Referential Transparency: Correct Functional Implementation of Forward-Mode AD. In Proceedings of the 17th International Workshop on Implementation and Application of Functional Languages (IFL2005), Dublin, Ireland, Sep. 19-21, 2005._\n",
"\n",
"_Jeffrey Mark Siskind and Barak A. Pearlmutter. Nesting forward-mode AD in a functional framework. Higher Order and Symbolic Computation 21(4):361-76, 2008. [doi:10.1007/s10990-008-9037-1](https://dx.doi.org/10.1007/s10990-008-9037-1) _\n",
"\n",
"_Barak A. Pearlmutter and Jeffrey Mark Siskind. Reverse-Mode AD in a functional framework: Lambda the ultimate backpropagator. TOPLAS 30(2):1-36, Mar. 2008. [doi:10.1145/1330017.1330018](https://dx.doi.org/10.1145/1330017.1330018) _\n",
"\n",
"Forward and Reverse AD Operations\n",
"---------------------------------\n",
"\n",
"DiffSharp automatically selects forward or reverse AD, or a combination of these, for a given operation.\n",
"\n",
"The following are just a small selection of operations.\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 5, "outputs": [], 
           "source": ["open DiffSharp.AD.Float64\n",
"\n",
"// f: D -\u003e D\n",
"let f (x:D) = sin (3. * sqrt x)\n",
"\n",
"// Derivative of f at 2\n",
"// Uses forward AD\n",
"let df = diff f (D 2.)\n",
"\n",
"// g: DV -\u003e D\n",
"let g (x:DV) = sin (x.[0] * x.[1])\n",
"\n",
"// Directional derivative of g at (2, 3) with direction (4, 1)\n",
"// Uses forward AD\n",
"let ddg = gradv g (toDV [2.; 3.]) (toDV [4.; 1.])\n",
"\n",
"// Gradient of g at (2, 3)\n",
"// Uses reverse AD\n",
"let gg = grad g (toDV [2.; 3.])\n",
"\n",
"// Hessian-vector product of g at (2, 3) with vector (4, 1)\n",
"// Uses reverse-on-forward AD\n",
"let hvg = hessianv g (toDV [2.; 3.]) (toDV [4.; 1.])\n",
"\n",
"// Hessian of g at (2, 3)\n",
"// Uses reverse-on-forward AD\n",
"let hg = hessian g (toDV [2.; 3.])\n",
"\n",
"// h: DV -\u003e DV\n",
"let h (x:DV) = toDV [sin x.[0]; cos x.[1]]\n",
"\n",
"// Jacobian-vector product of h at (2, 3) with vector (4, 1)\n",
"// Uses forward AD\n",
"let jvh = jacobianv h (toDV [2.; 3.]) (toDV [4.; 1.])\n",
"\n",
"// Transposed Jacobian-vector product of h at (2, 3) with vector (4, 1)\n",
"// Uses reverse AD\n",
"let tjvh = jacobianTv h (toDV [2.; 3.]) (toDV [4.; 1.])\n",
"\n",
"// Jacobian of h at (2, 3)\n",
"// Uses forward or reverse AD depending on the number of inputs and outputs\n",
"let jh = jacobian h (toDV [2.; 3.])\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["Using the Reverse AD Trace\n",
"--------------------------\n",
"\n",
"In addition to the high-level differentiation API that uses reverse AD (such as **grad**, **jacobianTv** ), you can make use of the exposed low-level [trace](https://en.wikipedia.org/wiki/Tracing_%28software%29) functionality. Reverse AD automatically builds a global trace (or \"tape\", in AD literature) of all executed numeric operations, which allows a subsequent reverse sweep of these operations for propagating adjoint values in reverse. \n",
"\n",
"The technique is equivalent to the [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) method commonly used for training artificial neural networks, which is essentially just a special case of reverse AD. (You can see an implementation of the backpropagation algorithm using reverse AD in the [neural networks example](examples-neuralnetworks.html).)\n",
"\n",
"For example, consider the computation\n",
"\n",
"\begin{equation}\n",
"  e = (\\sin a) (a + b) \\; ,\n",
"\\end{equation}\n",
"\n",
"using the values $a = 0.5$ and $b = 1.2$.\n",
"\n",
"During the execution of a program, this computation is carried out by the sequence of operations\n",
"\n",
"\begin{equation}\n",
" \\begin{eqnarray*}\n",
" a \u0026=\u0026 0.5 \\; ,\\\\\n",
" b \u0026=\u0026 1.2 \\; , \\\\\n",
" c \u0026=\u0026 \\sin a \\; , \\\\\n",
" d \u0026=\u0026 a + b \\; , \\\\\n",
" e \u0026=\u0026 c \\times d \\; ,\n",
" \\end{eqnarray*}\n",
"\\end{equation}\n",
"\n",
"the dependencies between which can be represented by the computational graph below.\n",
"\n",
"\u003cdiv class=\"row\"\u003e\n",
"\u003cdiv class=\"row\"\u003e\n",
"    \u003cdiv class=\"span6 offset2\"\u003e\n",
"        \u003cimg src=\"img/gettingstarted-reversead-graph.png\" alt=\"Chart\" style=\"width:350px;\"/\u003e\n",
"    \u003c/div\u003e\n",
"\u003c/div\u003e\n",
"Reverse mode AD works by propagating adjoint values from the output (e.g. $\\bar{e} = \\frac{\\partial e}{\\partial e}$) towards the inputs (e.g. $\\bar{a} = \\frac{\\partial e}{\\partial a}$ and $\\bar{b} = \\frac{\\partial e}{\\partial b}$), using adjoint propagation rules dictated by the dependencies in the computational graph:\n",
"\n",
"\begin{equation}\n",
" \\begin{eqnarray*}\n",
" \\bar{d} \u0026=\u0026 \\frac{\\partial e}{\\partial d} \u0026=\u0026 \\frac{\\partial e}{\\partial e} \\frac{\\partial e}{\\partial d} \u0026=\u0026 \\bar{e} c\\; , \\\\\n",
" \\bar{c} \u0026=\u0026 \\frac{\\partial e}{\\partial c} \u0026=\u0026 \\frac{\\partial e}{\\partial e} \\frac{\\partial e}{\\partial c} \u0026=\u0026 \\bar{e} d\\; , \\\\\n",
" \\bar{b} \u0026=\u0026 \\frac{\\partial e}{\\partial b} \u0026=\u0026 \\frac{\\partial e}{\\partial d} \\frac{\\partial d}{\\partial b} \u0026=\u0026 \\bar{d} \\; , \\\\\n",
" \\bar{a} \u0026=\u0026 \\frac{\\partial e}{\\partial a} \u0026=\u0026 \\frac{\\partial e}{\\partial c} \\frac{\\partial c}{\\partial a} + \\frac{\\partial e}{\\partial d} \\frac{\\partial d}{\\partial a} \u0026=\u0026 \\bar{c} (\\cos a) + \\bar{d} \\; .\\\\\n",
" \\end{eqnarray*}\n",
"\\end{equation}\n",
"  \n",
"In order to write code using low-level AD functionality, you should understand the tagging method used for avoiding perturbation confusion. Users would need to write such code sporadically. The normal way of interacting with the library is through the high-level differentiation API, which handles these issues internally.\n",
"\n",
"You can get access to adjoints as follows.\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 6, "outputs": [
          {
           "data": {
            "text/plain": ["val a : D = DR (D 0.5,Noop,219u)",
"",
"val b : D = DR (D 1.2,Noop,219u)",
"",
"val e : D =",
"",
"  DR",
"",
"    (D 0.8150234156,",
"",
"     Mul_D_D",
"",
"       (DR (D 0.4794255386,Sin_D (DR (D 0.5,Noop,219u)),219u),",
"",
"        DR (D 1.7,Add_D_D (DR (D 0.5,Noop,219u),DR (D 1.2,Noop,219u)),219u)),",
"",
"     219u)",
"",
"val adjoints : Adjoints",
"",
"val deda : D = D 1.971315894",
"",
"val dedb : D = D 0.4794255386"]
        },
           "execution_count": 6,
           "metadata": {},
           "output_type": "execute_result"
          }], 
           "source": ["open DiffSharp.AD.Float64\n",
"\n",
"// Get a fresh global tag for this run of reverse AD\n",
"let i = DiffSharp.Util.GlobalTagger.Next\n",
"\n",
"// Initialize input values for reverse AD\n",
"let a = D 0.5 |\u003e makeReverse i\n",
"let b = D 1.2 |\u003e makeReverse i\n",
"\n",
"// Perform a series of operations involving the D type\n",
"let e = (sin a) * (a + b)\n",
"\n",
"// Propagate the adjoint value of 1 backward from e (or de/de = 1)\n",
"// i.e., calculate partial derivatives of e with respect to other variables\n",
"let adjoints = e |\u003e computeAdjoints\n",
"\n",
"// Read the adjoint values of the inputs\n",
"// You can calculate all partial derivatives in just one reverse sweep!\n",
"let deda = adjoints.[a]\n",
"let dedb = adjoints.[b]\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["In addition to the partial derivatives of the dependent variable $e$ with respect to the independent variables $a$ and $b$, you can also extract the partial derivatives of $e$ with respect to any intermediate variable involved in this computation.\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 8, "outputs": [
          {
           "data": {
            "text/plain": ["val a\u0027 : D = DR (D 0.5,Noop,221u)",
"",
"val b\u0027 : D = DR (D 1.2,Noop,221u)",
"",
"val c\u0027 : D = DR (D 0.4794255386,Sin_D (DR (D 0.5,Noop,221u)),221u)",
"",
"val d\u0027 : D =",
"",
"  DR (D 1.7,Add_D_D (DR (D 0.5,Noop,221u),DR (D 1.2,Noop,221u)),221u)",
"",
"val e\u0027 : D =",
"",
"  DR",
"",
"    (D 0.8150234156,",
"",
"     Mul_D_D",
"",
"       (DR (D 0.4794255386,Sin_D (DR (D 0.5,Noop,221u)),221u),",
"",
"        DR (D 1.7,Add_D_D (DR (D 0.5,Noop,221u),DR (D 1.2,Noop,221u)),221u)),",
"",
"     221u)",
"",
"val adjoints\u0027 : Adjoints",
"",
"val de\u0027da\u0027 : D = D 1.971315894",
"",
"val de\u0027db\u0027 : D = D 0.4794255386",
"",
"val de\u0027dc\u0027 : D = D 1.7",
"",
"val de\u0027dd\u0027 : D = D 0.4794255386"]
        },
           "execution_count": 8,
           "metadata": {},
           "output_type": "execute_result"
          }], 
           "source": ["// Get a fresh global tag for this run of reverse AD\n",
"let i\u0027 = DiffSharp.Util.GlobalTagger.Next\n",
"\n",
"// Initialize input values for reverse AD\n",
"let a\u0027 = D 0.5 |\u003e makeReverse i\u0027\n",
"let b\u0027 = D 1.2 |\u003e makeReverse i\u0027\n",
"\n",
"// Perform a series of operations involving the D type\n",
"let c\u0027 = sin a\u0027\n",
"let d\u0027 = a\u0027 + b\u0027\n",
"let e\u0027 = c\u0027 * d\u0027 // e\u0027 = (sin a\u0027) * (a\u0027 + b\u0027)\n",
"\n",
"// Propagate the adjoint value of 1 backward from e\n",
"let adjoints\u0027 = e\u0027 |\u003e computeAdjoints\n",
"\n",
"// Read the adjoint values\n",
"// You can calculate all partial derivatives in just one reverse sweep!\n",
"let de\u0027da\u0027 = adjoints\u0027.[a\u0027]\n",
"let de\u0027db\u0027 = adjoints\u0027.[b\u0027]\n",
"let de\u0027dc\u0027 = adjoints\u0027.[c\u0027]\n",
"let de\u0027dd\u0027 = adjoints\u0027.[d\u0027]\n"]
          }],
            "metadata": {
            "kernelspec": {"display_name": ".NET (F#)", "language": "F#", "name": ".net-fsharp"},
            "langauge_info": {
        "file_extension": ".fs",
        "mimetype": "text/x-fsharp",
        "name": "C#",
        "pygments_lexer": "fsharp",
        "version": "4.5"
        }
        },
            "nbformat": 4,
            "nbformat_minor": 1
        }
        
